{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63389587-3e69-4acf-b67d-dad3238c6c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time#可以用来简单地记录时间\n",
    "import matplotlib.pyplot as plt#画图\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  average_precision_score\n",
    "\n",
    "import torch#深度学习的pytoch平台\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528835f6-22ca-4cc8-bd3f-54387697048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.itransformer import itransformerModel\n",
    "from models.lstm_itransformer import lstm_itransformerModel\n",
    "from models.lstm import LSTMModel\n",
    "# from trainer.train_and_evaluate_2 import ModelTrainer, ModelEvaluator\n",
    "# from data_provider.dataset_generate import TimeSeriesDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f504f-a9a9-4f0e-85e9-22cadd95aeab",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65a60c2-fa40-434d-b299-870f77a6877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimic ['ams', 'eicu', 'salz']\n"
     ]
    }
   ],
   "source": [
    "database_names_all = ['ams', 'eicu', 'mimic', 'salz']\n",
    "\n",
    "selected_database_num = 2\n",
    "internal_database = database_names_all[selected_database_num]  # ''\n",
    "external_database = database_names_all.copy()\n",
    "external_database.remove(internal_database) # []\n",
    "print(internal_database, external_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c468382-1bd9-40f2-9437-4d1cd3a0a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"E:\\\\Research\\\\Time series research\\\\Federated learning Time series research\\\\0.data\\\\Multi-center time series data\\\\\"\n",
    "internal_data_path = 'icu_mortality_' + internal_database + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70ad32a-c331-4cab-844a-9026b4c9bfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>bmi</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>death_hosp</th>\n",
       "      <th>los_icu_day</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>...</th>\n",
       "      <th>oasis</th>\n",
       "      <th>sapsii</th>\n",
       "      <th>respiration</th>\n",
       "      <th>coagulation</th>\n",
       "      <th>liver</th>\n",
       "      <th>cardiovascular</th>\n",
       "      <th>cns</th>\n",
       "      <th>renal</th>\n",
       "      <th>sofa</th>\n",
       "      <th>mods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.79298</td>\n",
       "      <td>-0.998913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.79298</td>\n",
       "      <td>-0.998913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.79298</td>\n",
       "      <td>-0.998913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.79298</td>\n",
       "      <td>-0.998913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.79298</td>\n",
       "      <td>-0.998913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918410</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.54191</td>\n",
       "      <td>-0.931153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918411</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.54191</td>\n",
       "      <td>-0.931153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918412</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.54191</td>\n",
       "      <td>-0.931153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918413</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.54191</td>\n",
       "      <td>-0.931153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918414</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.54191</td>\n",
       "      <td>-0.931153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3918415 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  gender       age    height   weight       bmi  admission_type  \\\n",
       "0            5       0  1.017520 -0.021828 -0.79298 -0.998913               1   \n",
       "1            5       0  1.017520 -0.021828 -0.79298 -0.998913               1   \n",
       "2            5       0  1.017520 -0.021828 -0.79298 -0.998913               1   \n",
       "3            5       0  1.017520 -0.021828 -0.79298 -0.998913               1   \n",
       "4            5       0  1.017520 -0.021828 -0.79298 -0.998913               1   \n",
       "...        ...     ...       ...       ...      ...       ...             ...   \n",
       "3918410  76271       1 -0.637632  0.583969 -0.54191 -0.931153               0   \n",
       "3918411  76271       1 -0.637632  0.583969 -0.54191 -0.931153               0   \n",
       "3918412  76271       1 -0.637632  0.583969 -0.54191 -0.931153               0   \n",
       "3918413  76271       1 -0.637632  0.583969 -0.54191 -0.931153               0   \n",
       "3918414  76271       1 -0.637632  0.583969 -0.54191 -0.931153               0   \n",
       "\n",
       "         death_hosp  los_icu_day ethnicity  ...  oasis  sapsii  respiration  \\\n",
       "0                 0         1.73     other  ...   33.0    35.0            0   \n",
       "1                 0         1.73     other  ...   33.0    37.0            0   \n",
       "2                 0         1.73     other  ...   33.0    37.0            0   \n",
       "3                 0         1.73     other  ...   35.0    37.0            0   \n",
       "4                 0         1.73     other  ...   33.0    35.0            0   \n",
       "...             ...          ...       ...  ...    ...     ...          ...   \n",
       "3918410           0         1.17     white  ...    6.0    21.0            2   \n",
       "3918411           0         1.17     white  ...    6.0    21.0            0   \n",
       "3918412           0         1.17     white  ...    6.0    21.0            0   \n",
       "3918413           0         1.17     white  ...    6.0    21.0            0   \n",
       "3918414           0         1.17     white  ...    6.0    21.0            0   \n",
       "\n",
       "         coagulation  liver  cardiovascular  cns  renal  sofa  mods  \n",
       "0                  1      0               1    4      4  10.0     0  \n",
       "1                  1      0               1    4      4  10.0     0  \n",
       "2                  1      0               1    4      4  10.0     0  \n",
       "3                  1      0               1    4      4  10.0     0  \n",
       "4                  1      0               1    4      4  10.0     0  \n",
       "...              ...    ...             ...  ...    ...   ...   ...  \n",
       "3918410            0      0               1    1      0   4.0     0  \n",
       "3918411            0      0               0    0      0   NaN     0  \n",
       "3918412            0      0               0    0      0   NaN     0  \n",
       "3918413            0      0               0    0      0   NaN     0  \n",
       "3918414            0      0               0    0      0   NaN     0  \n",
       "\n",
       "[3918415 rows x 76 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_internal = pd.read_csv(file_path + internal_data_path)\n",
    "df_internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49a59dd6-26f0-407e-aaf0-834a6b7441f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "death_hosp\n",
      "0    35436\n",
      "1     5614\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_internal.groupby('id')['death_hosp'].last().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96bd7a3-75d0-4d7c-9a14-99851e806406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40804, 52798, 37874, ..., 24565, 47758, 43247], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids, test_ids = train_test_split(\n",
    "    df_internal['id'].unique(),  # 按患者ID划分\n",
    "    test_size=0.2,      # 测试集比例\n",
    "    random_state=42,    # 随机种子\n",
    "    stratify=df_internal.groupby('id')['death_hosp'].last()  # 按患者最终标签分层\n",
    ")\n",
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a24066f-3eb9-40d9-b062-9234992a00a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集比例:\n",
      "death_hosp\n",
      "0    0.863246\n",
      "1    0.136754\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "测试集比例:\n",
      "death_hosp\n",
      "0    0.863216\n",
      "1    0.136784\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 获取划分后的DataFrame\n",
    "train_df = df_internal[df_internal['id'].isin(train_ids)]\n",
    "test_df = df_internal[df_internal['id'].isin(test_ids)]\n",
    "\n",
    "print(\"训练集比例:\")\n",
    "print(train_df.groupby('id')['death_hosp'].last().value_counts(normalize=True))\n",
    "print(\"\\n测试集比例:\")\n",
    "print(test_df.groupby('id')['death_hosp'].last().value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637fd405-4d60-49bb-9112-9726ed85bf31",
   "metadata": {},
   "source": [
    "## 数据生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16a218ae-b1f9-49da-badc-287be64eb101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, window_size=24, forecast_horizon=24, stride=1, \n",
    "                 mode='sliding', shuffle=True, label=['death_hosp'], random_state=42, max_len=None):\n",
    "        \"\"\"\n",
    "        初始化时间序列数据集\n",
    "        \n",
    "        参数:\n",
    "            df: 包含所有数据的DataFrame\n",
    "            feature_cols: 使用的特征列名列表\n",
    "            window_size: 输入序列长度\n",
    "            forecast_horizon: 预测时间范围\n",
    "            mode: 'sliding'滑动窗口，'cumulative'累积窗口，‘fix’固定时间窗口\n",
    "            shuffle: 是否打乱数据顺序\n",
    "            random_state: 随机种子\n",
    "            label: 标签\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.mode = mode\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.max_len = max_len\n",
    "        self.indices = []\n",
    "        self.stride = stride\n",
    "        self.label = label\n",
    "        \n",
    "\n",
    "        # if len(label)==1:\n",
    "        #     self.label = label[0]\n",
    "        # else:\n",
    "        #     self.label = label\n",
    "        \n",
    "        # 预计算所有可能的序列索引\n",
    "        self._precompute_indices()\n",
    "        \n",
    "    def _precompute_indices(self):\n",
    "        \"\"\"计算所有有效的序列索引\"\"\"\n",
    "        random.seed(self.random_state)\n",
    "        \n",
    "        for pid, group in tqdm(self.df.groupby('id')):\n",
    "            group = group.sort_values('hr')\n",
    "            max_hr = group['hr'].max()\n",
    "            \n",
    "            if self.mode == 'sliding':\n",
    "                for start in range(1, max_hr - self.window_size, self.stride):\n",
    "                    end = start + self.window_size-1\n",
    "                    forecast_end = end + self.forecast_horizon\n",
    "                    if len(group[(group['hr'] >= start) & (group['hr'] <= end)]) == self.window_size:\n",
    "                        y = []\n",
    "                        for label in self.label:\n",
    "                            condition = (group['hr'] >= end) & (group['hr'] <= forecast_end) & (group[label] == 1)\n",
    "                            y.append(int(condition.any()))\n",
    "                        self.indices.append((pid, start, end, y))\n",
    "                        \n",
    "            elif self.mode == 'cumulative':\n",
    "                # for end in range(max_hr, max_hr+1, self.stride):\n",
    "                # for end in range(13, 14, self.stride):\n",
    "                for end in range(1, max_hr + 1, self.stride):\n",
    "                    start = 1\n",
    "                    forecast_end = end + self.forecast_horizon\n",
    "                    y = []\n",
    "                    for label in self.label:\n",
    "                        condition = (group['hr'] >= end) & (group['hr'] <= forecast_end) & (group[label] == 1)\n",
    "                        y.append(int(condition.any()))\n",
    "                    self.indices.append((pid, start, end, y))\n",
    "                    \n",
    "            elif self.mode == 'fix':\n",
    "                start = 1\n",
    "                end = start + self.window_size\n",
    "                if len(group[(group['hr'] >= start) & (group['hr'] < end)]) == self.window_size:\n",
    "                    y = []\n",
    "                    for label in self.label:\n",
    "                        condition = (group['hr'] >= end) & (group[label] == 1)\n",
    "                        y.append(int(condition.any()))\n",
    "                    self.indices.append((pid, start, end, y))\n",
    "                        \n",
    "                    # # condition = (group['hr'] >= end) & (group['death_hosp'] == 1)\n",
    "                    # condition = group['death_hosp'] == 1\n",
    "                    # y = int(condition.any())\n",
    "                    # self.indices.append((pid, start, end, y))\n",
    "                \n",
    "        \n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pid, start, end, y = self.indices[idx]\n",
    "        group = self.df[self.df['id'] == pid].sort_values('hr')\n",
    "        \n",
    "        # 获取特征序列\n",
    "        X = group[(group['hr'] >= start) & (group['hr'] <= end)][self.feature_cols].values\n",
    "        \n",
    "        # 转换为torch张量\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "        if self.max_len is not None:\n",
    "            X_tensor = X_tensor[-self.max_len:]  # 截断到最大长度\n",
    "            \n",
    "        seq_len = len(X_tensor)\n",
    "        \n",
    "        return X_tensor, y_tensor, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b85c7e48-5bc8-4546-a859-61bb91f34b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 32840/32840 [01:55<00:00, 284.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 8210/8210 [00:29<00:00, 274.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# 定义特征列\n",
    "feature_cols = ['gender', 'age', 'height', 'weight',\n",
    "                'heart_rate', 'sbp', 'mbp', 'dbp', 'resp_rate', 'temperature', 'spo2', 'albumin', 'aniongap', 'bun', 'calcium', 'chloride', \n",
    "                'creatinine', 'glucose', 'sodium', 'potassium', 'fibrinogen', 'inr', 'pt', 'ptt', 'hematocrit', 'hemoglobin', 'platelet', 'wbc', \n",
    "                'alt', 'ast', 'bilirubin', 'pao2', 'paco2', 'fio2', 'pao2fio2ratio', 'ph', 'baseexcess', 'lactate', 'sao2', 'troponin', 'magnesium', \n",
    "                'bnp', 'neutrophils', 'gcs', 'alkaline_phosphatase', 'norepinephrine', 'epinephrine', 'dobutamine', 'dopamine', 'ventilation',\n",
    "                'lymphocytes', 'bicarbonate', 'urineoutput',\n",
    "               ]\n",
    "\n",
    "\n",
    "# feature_cols = ['heart_rate', 'resp_rate', 'spo2', 'temperature', 'sbp', 'mbp', 'urineoutput', 'albumin',\n",
    "#  'calcium', 'bilirubin', 'bun', 'creatinine', 'glucose', 'bicarbonate', 'hematocrit', 'hemoglobin', 'lactate', 'platelet', 'ptt', 'sodium', 'wbc',\n",
    "#  'alt', 'ast', 'alkaline_phosphatase', 'inr', 'fio2', 'pao2', 'pao2fio2ratio', 'paco2', 'ph', 'aniongap', 'baseexcess', 'troponin', 'magnesium',\n",
    "#  'bnp', 'fibrinogen', 'lymphocytes', 'chloride', 'pt', 'neutrophils',\n",
    "#                ]\n",
    "    \n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = TimeSeriesDataset(train_df, feature_cols, window_size=24, forecast_horizon=24, mode='sliding', stride=4, shuffle=False)\n",
    "test_dataset = TimeSeriesDataset(test_df, feature_cols, window_size=24, forecast_horizon=24, mode='sliding', stride=4, shuffle=False)\n",
    "\n",
    "# train_dataset = TimeSeriesDataset(train_df, feature_cols, window_size=24, mode='fix', shuffle=False)\n",
    "# test_dataset = TimeSeriesDataset(test_df, feature_cols, window_size=24, mode='fix', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35769a8f-61b9-48ab-997e-8ef0d3f7f61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "使用numpy统计:\n",
      "0: 565868 个 (96.1%)\n",
      "1: 22996 个 (3.9%)\n"
     ]
    }
   ],
   "source": [
    "# 提取标签列\n",
    "ys = np.array([y for (pid, start, end, y) in train_dataset.indices])\n",
    "unique, counts = np.unique(ys, return_counts=True)\n",
    "label_dist = dict(zip(unique, counts))\n",
    "\n",
    "print(\"\\n使用numpy统计:\")\n",
    "for label, count in label_dist.items():\n",
    "    print(f\"{label}: {count} 个 ({(count/len(ys))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2b55c64-68a8-41b9-844f-e214e578bc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "使用numpy统计:\n",
      "0: 144733 个 (96.1%)\n",
      "1: 5880 个 (3.9%)\n"
     ]
    }
   ],
   "source": [
    "# 提取标签列\n",
    "ys = np.array([y for (pid, start, end, y) in test_dataset.indices])\n",
    "unique, counts = np.unique(ys, return_counts=True)\n",
    "label_dist = dict(zip(unique, counts))\n",
    "\n",
    "print(\"\\n使用numpy统计:\")\n",
    "for label, count in label_dist.items():\n",
    "    print(f\"{label}: {count} 个 ({(count/len(ys))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08a41695-0307-4777-9be2-fd6341b95ef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 1, 25, [0]),\n",
       " (5, 5, 29, [0]),\n",
       " (5, 9, 33, [0]),\n",
       " (5, 13, 37, [0]),\n",
       " (5, 17, 41, [0]),\n",
       " (10, 1, 25, [0]),\n",
       " (10, 5, 29, [0]),\n",
       " (10, 9, 33, [0]),\n",
       " (10, 13, 37, [0]),\n",
       " (10, 17, 41, [0])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0ab21a8-9547-45d3-ae3c-acad52a5d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    自定义collate函数处理变长序列，生成padding mask\n",
    "    \"\"\"\n",
    "    sequences, targets, lengths = zip(*batch)\n",
    "    # print(targets)\n",
    "    \n",
    "    # 按序列长度排序(降序)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    lengths, sort_idx = lengths.sort(descending=True)\n",
    "    sequences = [sequences[i] for i in sort_idx]\n",
    "    targets = torch.stack([targets[i] for i in sort_idx])\n",
    "    \n",
    "    # 填充序列 (batch_first=True)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # 创建padding mask (1表示有效数据，0表示padding)\n",
    "    batch_size, max_len = sequences_padded.shape[0], sequences_padded.shape[1]\n",
    "    padding_mask = torch.arange(max_len).expand(batch_size, max_len) < lengths.unsqueeze(1)\n",
    "    padding_mask = padding_mask.float().unsqueeze(-1)  # (batch_size, max_len, 1)\n",
    "    \n",
    "    return sequences_padded, targets, padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "070f7480-6fc8-4967-9759-b5af74312095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "064a71b5-3aa0-4305-9de9-488319b86f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588864"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee4de0d0-be28-412b-8c65-0161af8fe887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 24, 53])\n"
     ]
    }
   ],
   "source": [
    "for x, y, padding_mask in train_loader:\n",
    "    print(x.shape)\n",
    "    # print(y)\n",
    "    # print(padding_mask)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0debf98e-623a-448a-83c3-505a440444fc",
   "metadata": {},
   "source": [
    "## 检查设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e70b8633-5430-47c3-93bf-1a29ce46429d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are %d GPU(s) available. 1\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080\n"
     ]
    }
   ],
   "source": [
    "# Check device \n",
    "# Get the GPU device name if available.\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available. {}'.format(torch.cuda.device_count()))\n",
    "    print('We will use the GPU: {}'.format(torch.cuda.get_device_name(0)))\n",
    "\n",
    "# If we dont have GPU but a CPU, training will take place on CPU instead\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "    \n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a13d75-6661-4946-9932-4a3ab80bcb01",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99b61bb8-6ee6-4b63-a2dc-0d7229a73aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model = itransformerModel(seq_len=24, d_model=128, d_ff=256, e_layers=3, enc_in=len(feature_cols), num_class=2)\n",
    "# model = lstm_itransformerModel(seq_len=24, d_model=128, d_ff=256, e_layers=3, enc_in=len(feature_cols), num_class=2)\n",
    "# # 初始化模型\n",
    "# model = LSTMModel(input_size=len(feature_cols), hidden_size=64, num_layers=2, dropout=0.1, num_class=2)\n",
    "\n",
    "# model_path = './checkpoint.pth'\n",
    "# model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c248e1d-af07-48c7-9dac-b8c6473d325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e7ad7f8-e1dd-4900-bbee-02a6a6d6f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ae8725-cf57-4e13-ac61-81affabcbb23",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74b223be-71f9-449e-9cae-0c1eb18106b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  average_precision_score\n",
    "\n",
    "# 训练与评估框架\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer,device='cuda'):\n",
    "        \"\"\"\n",
    "        初始化训练器\n",
    "        参数:\n",
    "            model: 模型实例\n",
    "            train_loader: 训练数据加载器\n",
    "            val_loader: 验证数据加载器\n",
    "            device: 训练设备\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.best_auc = 0\n",
    "        self.best_model = None\n",
    "        \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"训练一个epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(self.train_loader, desc=f\"Epoch {epoch + 1} [Train]\")\n",
    "        \n",
    "        for X, y, padding_mask in progress_bar:\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            padding_mask = padding_mask.to(self.device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = self.model(X,padding_mask)\n",
    "            if y.shape[1] == 1:\n",
    "                loss = self.criterion(outputs, y.long().squeeze(-1))\n",
    "                # loss = self.criterion(outputs, y)\n",
    "            else:\n",
    "                loss = torch.tensor(0.0, device=self.device)\n",
    "                for i in range(len(outputs)):\n",
    "                    loss = loss + self.criterion(outputs[i], y[:, i].long().squeeze(-1))\n",
    "            # # loss = self.criterion(outputs, y)\n",
    "            # loss = self.criterion(outputs, y.long().squeeze(-1))\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=4.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        return total_loss / len(self.train_loader.dataset)\n",
    "    \n",
    "    def validate(self, testloader):\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        # total_loss = []\n",
    "        # preds = []\n",
    "        # trues = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y, padding_mask in tqdm(testloader, desc=\"Validating\"):\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                padding_mask = padding_mask.to(self.device)\n",
    "                outputs = self.model(X,padding_mask)\n",
    "                if y.shape[1] == 1:\n",
    "                    loss = self.criterion(outputs, y.long().squeeze(-1))\n",
    "                    # loss = self.criterion(outputs, y)\n",
    "                else:\n",
    "                    loss = torch.tensor(0.0, device=self.device)\n",
    "                    for i in range(len(outputs)):\n",
    "                        loss = loss + self.criterion(outputs[i], y[:, i].long().squeeze(-1))\n",
    "                # loss = self.criterion(outputs, y)\n",
    "                \n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "        \n",
    "        val_loss /= len(testloader.dataset)\n",
    "        val_auc = roc_auc_score(all_labels, np.array(all_preds)[:,1])\n",
    "        val_auprc = average_precision_score(all_labels, np.array(all_preds)[:,1])\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_auc > self.best_auc:\n",
    "            self.best_auc = val_auc\n",
    "            self.best_model = self.model.state_dict().copy()\n",
    "        \n",
    "        return val_loss, val_auc, val_auprc, np.array(all_preds), all_labels\n",
    "    \n",
    "    def train(self, num_epochs=50, early_stop_patience=5):\n",
    "        \"\"\"完整训练流程\"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_aucs = []\n",
    "        \n",
    "        no_improve = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            val_loss, val_auc, val_auprc = self.validate(self.val_loader)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_aucs.append(val_auc)\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Val AUC: {val_auc:.4f} | Val AUPRC: {val_auprc:.4f}\")\n",
    "\n",
    "            torch.save(self.model.state_dict(), f'weights_lstmitransformer_cum_24_2_{epoch}epoch.pth')\n",
    "            # # 早停机制\n",
    "            # if val_auc > self.best_auc:\n",
    "            #     no_improve = 0\n",
    "            # else:\n",
    "            #     no_improve += 1\n",
    "            #     if no_improve >= early_stop_patience:\n",
    "            #         print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            #         break\n",
    "        \n",
    "        # 加载最佳模型\n",
    "        self.model.load_state_dict(self.best_model)\n",
    "        return train_losses, val_losses, val_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c01a6908-f10b-434b-93f3-0d5b5e999b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "# criterion = nn.BCELoss()\n",
    "# weight = torch.tensor([0.025, 0.975]).to(device)\n",
    "# criterion =  nn.CrossEntropyLoss(weight=weight)\n",
    "criterion =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# trainer = ModelTrainer(model, train_loader, test_loader, test_loader , criterion, optimizer)\n",
    "# train_losses, val_losses, val_aucs = trainer.train(num_epochs=5)\n",
    "# total_loss, accuracy, auroc, auprc = trainer.evaluate(test_loader)\n",
    "\n",
    "# 训练模型\n",
    "trainer = ModelTrainer(model, train_loader, test_loader, criterion, optimizer)\n",
    "# train_losses, val_losses, val_aucs = trainer.train(num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8482dc7c-9985-4d8e-b39b-ee7b92f6759a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses, val_losses, val_aucs\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, val_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9be00634-a7c3-4d9d-b5f7-f0a38fa7aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'weights_itransformer_24_24_2_new.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba88be-70a9-435b-b156-c258fb50d36f",
   "metadata": {},
   "source": [
    "# 外部验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84d9835c-0398-413b-8f02-4cecc1c7ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, accuracy_score, f1_score, precision_score, average_precision_score\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_all_metrics(all_preds, all_labels, threshold=0.5, n_bootstrap=1000, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    计算所有评估指标，包括Bootstrap置信区间\n",
    "    \n",
    "    参数:\n",
    "    -----------\n",
    "    all_preds : array-like\n",
    "        预测概率值\n",
    "    all_labels : array-like\n",
    "        真实标签\n",
    "    threshold : float, default=0.5\n",
    "        二分类阈值\n",
    "    n_bootstrap : int, default=1000\n",
    "        Bootstrap采样次数\n",
    "    confidence_level : float, default=0.95\n",
    "        置信水平\n",
    "    \n",
    "    返回:\n",
    "    -----------\n",
    "    metrics_dict : dict\n",
    "        包含所有指标及其置信区间的字典\n",
    "    \"\"\"\n",
    "    # 将概率转换为二分类预测\n",
    "    binary_preds = (all_preds >= threshold).astype(int)\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, binary_preds).ravel()\n",
    "    \n",
    "    # 原始指标计算\n",
    "    metrics = {}\n",
    "    \n",
    "    # AUROC\n",
    "    try:\n",
    "        metrics['AUROC'] = roc_auc_score(all_labels, all_preds)\n",
    "    except:\n",
    "        metrics['AUROC'] = np.nan\n",
    "    \n",
    "    # Sensitivity (Recall)\n",
    "    try:\n",
    "        metrics['Sensitivity'] = recall_score(all_labels, binary_preds, zero_division=0)\n",
    "    except:\n",
    "        metrics['Sensitivity'] = np.nan\n",
    "    \n",
    "    # Specificity\n",
    "    try:\n",
    "        metrics['Specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    except:\n",
    "        metrics['Specificity'] = np.nan\n",
    "    \n",
    "    # Accuracy\n",
    "    try:\n",
    "        metrics['Accuracy'] = accuracy_score(all_labels, binary_preds)\n",
    "    except:\n",
    "        metrics['Accuracy'] = np.nan\n",
    "    \n",
    "    # F1 score\n",
    "    try:\n",
    "        metrics['F1_score'] = f1_score(all_labels, binary_preds, zero_division=0)\n",
    "    except:\n",
    "        metrics['F1_score'] = np.nan\n",
    "    \n",
    "    # Precision\n",
    "    try:\n",
    "        metrics['Precision'] = precision_score(all_labels, binary_preds, zero_division=0)\n",
    "    except:\n",
    "        metrics['Precision'] = np.nan\n",
    "    \n",
    "    # AUPRC\n",
    "    try:\n",
    "        metrics['AUPRC'] = average_precision_score(all_labels, all_preds)\n",
    "    except:\n",
    "        metrics['AUPRC'] = np.nan\n",
    "    \n",
    "    # 额外添加一些可能有用的指标\n",
    "    metrics['TP'] = tp\n",
    "    metrics['FP'] = fp\n",
    "    metrics['TN'] = tn\n",
    "    metrics['FN'] = fn\n",
    "    \n",
    "    # 如果需要Bootstrap，计算置信区间\n",
    "    if n_bootstrap > 0:\n",
    "        bootstrap_metrics = _bootstrap_metrics(all_preds, all_labels, threshold, n_bootstrap, confidence_level)\n",
    "        \n",
    "        # 合并原始指标和Bootstrap结果\n",
    "        result_dict = {}\n",
    "        for key in metrics.keys():\n",
    "            result_dict[key] = {\n",
    "                'value': metrics[key],\n",
    "                'ci_lower': bootstrap_metrics.get(f'{key}_ci_lower', np.nan),\n",
    "                'ci_upper': bootstrap_metrics.get(f'{key}_ci_upper', np.nan),\n",
    "                'mean': bootstrap_metrics.get(f'{key}_mean', np.nan),\n",
    "                'std': bootstrap_metrics.get(f'{key}_std', np.nan)\n",
    "            }\n",
    "        \n",
    "        return result_dict\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def _bootstrap_metrics(all_preds, all_labels, threshold=0.5, n_bootstrap=1000, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Bootstrap采样计算指标置信区间\n",
    "    \n",
    "    参数:\n",
    "    -----------\n",
    "    all_preds : array-like\n",
    "        预测概率值\n",
    "    all_labels : array-like\n",
    "        真实标签\n",
    "    threshold : float, default=0.5\n",
    "        二分类阈值\n",
    "    n_bootstrap : int, default=1000\n",
    "        Bootstrap采样次数\n",
    "    confidence_level : float, default=0.95\n",
    "        置信水平\n",
    "    \n",
    "    返回:\n",
    "    -----------\n",
    "    bootstrap_results : dict\n",
    "        Bootstrap计算的所有指标统计量\n",
    "    \"\"\"\n",
    "    n_samples = len(all_labels)\n",
    "    indices = np.arange(n_samples)\n",
    "    \n",
    "    # 存储所有Bootstrap样本的指标\n",
    "    bootstrap_auroc = []\n",
    "    bootstrap_sensitivity = []\n",
    "    bootstrap_specificity = []\n",
    "    bootstrap_accuracy = []\n",
    "    bootstrap_f1 = []\n",
    "    bootstrap_precision = []\n",
    "    bootstrap_auprc = []\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        # 有放回抽样\n",
    "        bootstrap_indices = np.random.choice(indices, size=n_samples, replace=True)\n",
    "        \n",
    "        # 获取Bootstrap样本\n",
    "        bootstrap_preds = all_preds[bootstrap_indices]\n",
    "        bootstrap_labels = all_labels[bootstrap_indices]\n",
    "        \n",
    "        # 转换为二分类预测\n",
    "        binary_preds = (bootstrap_preds >= threshold).astype(int)\n",
    "        \n",
    "        try:\n",
    "            # 计算混淆矩阵\n",
    "            tn, fp, fn, tp = confusion_matrix(bootstrap_labels, binary_preds).ravel()\n",
    "            \n",
    "            # 计算各项指标\n",
    "            # AUROC\n",
    "            try:\n",
    "                bootstrap_auroc.append(roc_auc_score(bootstrap_labels, bootstrap_preds))\n",
    "            except:\n",
    "                bootstrap_auroc.append(np.nan)\n",
    "            \n",
    "            # Sensitivity\n",
    "            try:\n",
    "                bootstrap_sensitivity.append(recall_score(bootstrap_labels, binary_preds, zero_division=0))\n",
    "            except:\n",
    "                bootstrap_sensitivity.append(np.nan)\n",
    "            \n",
    "            # Specificity\n",
    "            try:\n",
    "                bootstrap_specificity.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "            except:\n",
    "                bootstrap_specificity.append(np.nan)\n",
    "            \n",
    "            # Accuracy\n",
    "            try:\n",
    "                bootstrap_accuracy.append(accuracy_score(bootstrap_labels, binary_preds))\n",
    "            except:\n",
    "                bootstrap_accuracy.append(np.nan)\n",
    "            \n",
    "            # F1 score\n",
    "            try:\n",
    "                bootstrap_f1.append(f1_score(bootstrap_labels, binary_preds, zero_division=0))\n",
    "            except:\n",
    "                bootstrap_f1.append(np.nan)\n",
    "            \n",
    "            # Precision\n",
    "            try:\n",
    "                bootstrap_precision.append(precision_score(bootstrap_labels, binary_preds, zero_division=0))\n",
    "            except:\n",
    "                bootstrap_precision.append(np.nan)\n",
    "            \n",
    "            # AUPRC\n",
    "            try:\n",
    "                bootstrap_auprc.append(average_precision_score(bootstrap_labels, bootstrap_preds))\n",
    "            except:\n",
    "                bootstrap_auprc.append(np.nan)\n",
    "                \n",
    "        except:\n",
    "            # 如果Bootstrap样本出现问题，添加NaN值\n",
    "            bootstrap_auroc.append(np.nan)\n",
    "            bootstrap_sensitivity.append(np.nan)\n",
    "            bootstrap_specificity.append(np.nan)\n",
    "            bootstrap_accuracy.append(np.nan)\n",
    "            bootstrap_f1.append(np.nan)\n",
    "            bootstrap_precision.append(np.nan)\n",
    "            bootstrap_auprc.append(np.nan)\n",
    "    \n",
    "    # 计算置信区间\n",
    "    alpha = 1 - confidence_level\n",
    "    ci_lower = alpha / 2 * 100\n",
    "    ci_upper = (1 - alpha / 2) * 100\n",
    "    \n",
    "    # 将列表转换为数组，过滤NaN值\n",
    "    def get_stats(values, metric_name):\n",
    "        values_array = np.array(values)\n",
    "        valid_values = values_array[~np.isnan(values_array)]\n",
    "        \n",
    "        if len(valid_values) == 0:\n",
    "            return {\n",
    "                f'{metric_name}_mean': np.nan,\n",
    "                f'{metric_name}_std': np.nan,\n",
    "                f'{metric_name}_ci_lower': np.nan,\n",
    "                f'{metric_name}_ci_upper': np.nan\n",
    "            }\n",
    "        \n",
    "        mean_val = np.mean(valid_values)\n",
    "        std_val = np.std(valid_values)\n",
    "        \n",
    "        # 使用百分位数法计算置信区间\n",
    "        ci_lower_val = np.percentile(valid_values, ci_lower)\n",
    "        ci_upper_val = np.percentile(valid_values, ci_upper)\n",
    "        \n",
    "        return {\n",
    "            f'{metric_name}_mean': mean_val,\n",
    "            f'{metric_name}_std': std_val,\n",
    "            f'{metric_name}_ci_lower': ci_lower_val,\n",
    "            f'{metric_name}_ci_upper': ci_upper_val\n",
    "        }\n",
    "    \n",
    "    # 收集所有指标的Bootstrap结果\n",
    "    bootstrap_results = {}\n",
    "    \n",
    "    # 合并各个指标的统计结果\n",
    "    for metric_name, values in [\n",
    "        ('AUROC', bootstrap_auroc),\n",
    "        ('Sensitivity', bootstrap_sensitivity),\n",
    "        ('Specificity', bootstrap_specificity),\n",
    "        ('Accuracy', bootstrap_accuracy),\n",
    "        ('F1_score', bootstrap_f1),\n",
    "        ('Precision', bootstrap_precision),\n",
    "        ('AUPRC', bootstrap_auprc)\n",
    "    ]:\n",
    "        bootstrap_results.update(get_stats(values, metric_name))\n",
    "    \n",
    "    return bootstrap_results\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4eda819-9356-4ae7-bac9-6dc19bc99a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235970, 68)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1247/1247 [00:09<00:00, 125.93it/s]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 3230/3230 [00:38<00:00, 84.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss, auroc, auprc:  0.11907117988423131 0.8222297972557429 0.2729485432874828\n"
     ]
    }
   ],
   "source": [
    "external_data_path = 'icu_mortality_' + 'zhejiang' + '.csv'\n",
    "df_external = pd.read_csv(file_path + external_data_path)\n",
    "print(df_external.shape)\n",
    "\n",
    "# df_external = pd.concat([df_external, df_external_1], axis=0)\n",
    "# print(df_external.shape)\n",
    "\n",
    "external_dataset = TimeSeriesDataset(df_external, feature_cols, window_size=24, forecast_horizon=24, mode='sliding', stride=4, shuffle=False)\n",
    "external_loader = DataLoader(external_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "total_loss, auroc, auprc, all_preds, all_labels = trainer.validate(external_loader)\n",
    "print(\"total_loss, auroc, auprc: \", total_loss, auroc, auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "961f7b16-400a-4a78-87d2-4db610a590c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asia: \n",
      "\n",
      "{'AUROC': {'value': 0.8222297972557429, 'ci_lower': 0.8098901801725736, 'ci_upper': 0.8318340010935624, 'mean': 0.8218920359753694, 'std': 0.006035780951919125}, 'Sensitivity': {'value': 0.74609375, 'ci_lower': 0.7247345106825404, 'ci_upper': 0.7663778204377589, 'mean': 0.7456360858648753, 'std': 0.010572740270272342}, 'Specificity': {'value': 0.739662910142615, 'ci_lower': 0.7364000750868435, 'ci_upper': 0.7437032100885013, 'mean': 0.7398900849467074, 'std': 0.0018676696524967732}, 'Accuracy': {'value': 0.7398540767548528, 'ci_lower': 0.7369477076116197, 'ci_upper': 0.7436318244276287, 'mean': 0.7400607690967854, 'std': 0.0018008286753232981}, 'F1_score': {'value': 0.14567179356806914, 'ci_lower': 0.14030438320112387, 'ci_upper': 0.1523311024720001, 'mean': 0.14607666496736663, 'std': 0.0034035270735293666}, 'Precision': {'value': 0.08071559374559797, 'ci_lower': 0.07739005959181337, 'ci_upper': 0.08461531928946613, 'mean': 0.08097395714868622, 'std': 0.0020372820396734365}, 'AUPRC': {'value': 0.2729485432874828, 'ci_lower': 0.24961562035529974, 'ci_upper': 0.29661163964036247, 'mean': 0.2724422137018352, 'std': 0.012494128400545831}, 'TP': {'value': 1146, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}, 'FP': {'value': 13052, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}, 'TN': {'value': 37083, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}, 'FN': {'value': 390, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}}\n"
     ]
    }
   ],
   "source": [
    "print( \"Asia: \\n\") \n",
    "print(calculate_all_metrics(np.array(all_preds)[:, 1].reshape(-1), np.array(all_labels).reshape(-1), threshold=-1.45, n_bootstrap=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad939d23-78b6-4385-9cc2-b42d6ea8b087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 2354/2354 [02:22<00:00, 16.55it/s]\n"
     ]
    }
   ],
   "source": [
    "total_loss, auroc, auprc, all_preds, all_labels = trainer.validate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1ddd466-2635-410f-bf15-4032eabb8035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal test set: \n",
      "\n",
      "{'AUROC': {'value': 0.8450462030694005, 'ci_lower': 0.8390794930954066, 'ci_upper': 0.8506218120622188, 'mean': 0.8451303790193903, 'std': 0.0031645735937166804}, 'Sensitivity': {'value': 0.7741496598639456, 'ci_lower': 0.7638290365657506, 'ci_upper': 0.7848504108404685, 'mean': 0.7744291037842155, 'std': 0.005791923122063964}, 'Specificity': {'value': 0.7399418239102347, 'ci_lower': 0.737801835107175, 'ci_upper': 0.7415773166798781, 'mean': 0.7397974489626225, 'std': 0.0010873045558060924}, 'Accuracy': {'value': 0.7412773133793231, 'ci_lower': 0.7390719924574903, 'ci_upper': 0.7430362916879685, 'mean': 0.7411488384136827, 'std': 0.0011092986403854258}, 'F1_score': {'value': 0.18938653242079423, 'ci_lower': 0.18446336530618812, 'ci_upper': 0.19376072481656859, 'mean': 0.1892952040064326, 'std': 0.0023948198195019436}, 'Precision': {'value': 0.10789030835960275, 'ci_lower': 0.10476699433771802, 'ci_upper': 0.11061377587861347, 'mean': 0.10782755519171161, 'std': 0.0014961133842488572}, 'AUPRC': {'value': 0.4209691178700375, 'ci_lower': 0.40687010164081516, 'ci_upper': 0.43504432814722416, 'mean': 0.4206330690095889, 'std': 0.007019944418502318}, 'TP': {'value': 4552, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}, 'FP': {'value': 37639, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}, 'TN': {'value': 107094, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}, 'FN': {'value': 1328, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}}\n"
     ]
    }
   ],
   "source": [
    "print( \"Internal test set: \\n\") \n",
    "print(calculate_all_metrics(np.array(all_preds)[:, 1].reshape(-1), np.array(all_labels).reshape(-1), threshold=-1.45, n_bootstrap=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2013c53d-55ba-4166-bf75-efde5bb9e479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(341758, 75)\n",
      "(685979, 75)\n",
      "(1027737, 75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9084/9084 [00:39<00:00, 231.78it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 12719/12719 [04:38<00:00, 45.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss, auroc, auprc:  0.11151691214863829 0.806134165340889 0.30206729843716484\n"
     ]
    }
   ],
   "source": [
    "external_data_path = 'icu_mortality_' + 'ams' + '.csv'\n",
    "df_external_1 = pd.read_csv(file_path + external_data_path)\n",
    "print(df_external_1.shape)\n",
    "\n",
    "external_data_path = 'icu_mortality_' + 'salz' + '.csv'\n",
    "df_external = pd.read_csv(file_path + external_data_path)\n",
    "print(df_external.shape)\n",
    "\n",
    "# df_external = pd.concat([df_external, df_external_1], axis=0)\n",
    "# print(df_external.shape)\n",
    "\n",
    "# 计算df1的最大id，并顺延df2的id\n",
    "max_id = df_external_1['id'].max()\n",
    "df_external['id'] = df_external['id'] + max_id\n",
    "\n",
    "# 合并DataFrame\n",
    "df_external = pd.concat([df_external_1, df_external], ignore_index=True)\n",
    "print(df_external.shape)\n",
    "\n",
    "external_dataset = TimeSeriesDataset(df_external, feature_cols, window_size=24, forecast_horizon=24, mode='sliding', stride=4, shuffle=False)\n",
    "external_loader = DataLoader(external_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "total_loss, auroc, auprc, all_preds, all_labels = trainer.validate(external_loader)\n",
    "print(\"total_loss, auroc, auprc: \", total_loss, auroc, auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15b2474e-47ba-42ca-b7df-9094eb910bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Europe: \n",
      "\n",
      "{'AUROC': {'value': 0.806134165340889, 'ci_lower': 0.7982142039695521, 'ci_upper': 0.81247908567915, 'mean': 0.8060668669631604, 'std': 0.0034800816913499537}, 'Sensitivity': {'value': 0.7432337434094903, 'ci_lower': 0.7310962290776154, 'ci_upper': 0.7525579067070758, 'mean': 0.7427227950448837, 'std': 0.005602932552064553}, 'Specificity': {'value': 0.7031348738922283, 'ci_lower': 0.701626733781057, 'ci_upper': 0.7050040886423092, 'mean': 0.7031463740460333, 'std': 0.0008679063078547409}, 'Accuracy': {'value': 0.7042560823992491, 'ci_lower': 0.7027392295709519, 'ci_upper': 0.7061283458724207, 'mean': 0.7042528882489668, 'std': 0.000837579763460565}, 'F1_score': {'value': 0.12322081554755904, 'ci_lower': 0.12002293306200451, 'ci_upper': 0.12583926106121193, 'mean': 0.12314444853414193, 'std': 0.0015659220921480272}, 'Precision': {'value': 0.06717923464281743, 'ci_lower': 0.06533105991929428, 'ci_upper': 0.06870043726367249, 'mean': 0.06713883350123913, 'std': 0.000909440737397029}, 'AUPRC': {'value': 0.30206729843716484, 'ci_lower': 0.28934728835721507, 'ci_upper': 0.31635362758914476, 'mean': 0.301697100732731, 'std': 0.006854199935689096}, 'TP': {'value': 4229, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}, 'FP': {'value': 58722, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}, 'TN': {'value': 139085, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}, 'FN': {'value': 1461, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}}\n"
     ]
    }
   ],
   "source": [
    "print( \"Europe: \\n\") \n",
    "print(calculate_all_metrics(np.array(all_preds)[:, 1].reshape(-1), np.array(all_labels).reshape(-1), threshold=-1.5, n_bootstrap=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "611d69f1-63ca-4877-8243-c9399bae509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3084332, 76)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 36768/36768 [01:48<00:00, 338.73it/s]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 8672/8672 [21:12<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss, auroc, auprc:  0.13613525099675716 0.7825795180021742 0.21246811829090967\n"
     ]
    }
   ],
   "source": [
    "external_data_path = 'icu_mortality_' + 'eicu' + '.csv'\n",
    "df_external = pd.read_csv(file_path + external_data_path)\n",
    "print(df_external.shape)\n",
    "\n",
    "# df_external = pd.concat([df_external, df_external_1], axis=0)\n",
    "# print(df_external.shape)\n",
    "\n",
    "external_dataset = TimeSeriesDataset(df_external, feature_cols, window_size=24, forecast_horizon=24, mode='sliding', stride=4, shuffle=False)\n",
    "external_loader = DataLoader(external_dataset, batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "total_loss, auroc, auprc, all_preds, all_labels = trainer.validate(external_loader)\n",
    "print(\"total_loss, auroc, auprc: \", total_loss, auroc, auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7fc0f349-4986-4f63-b953-0d15948d63e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US: \n",
      "\n",
      "{'AUROC': {'value': 0.7825795180021742, 'ci_lower': 0.7791226730539866, 'ci_upper': 0.7861272870439441, 'mean': 0.7826854150559829, 'std': 0.0016939289656860243}, 'Sensitivity': {'value': 0.7280341257868179, 'ci_lower': 0.7222529010220227, 'ci_upper': 0.7349259231936381, 'mean': 0.7281368273644265, 'std': 0.0030468932569052824}, 'Specificity': {'value': 0.6816399213787465, 'ci_lower': 0.6803776531063915, 'ci_upper': 0.6829024958885559, 'mean': 0.6815684790520361, 'std': 0.0006630847033232283}, 'Accuracy': {'value': 0.6832469601193608, 'ci_lower': 0.6820766871607838, 'ci_upper': 0.6844557946936334, 'mean': 0.6831831172201039, 'std': 0.0006432259602059428}, 'F1_score': {'value': 0.13735805352989094, 'ci_lower': 0.13554105971341113, 'ci_upper': 0.13931725231371622, 'mean': 0.13746402970755647, 'std': 0.0010757086043705292}, 'Precision': {'value': 0.07583269665295772, 'ci_lower': 0.07472047384706108, 'ci_upper': 0.07700789418861624, 'mean': 0.07589658883710886, 'std': 0.0006398350222791456}, 'AUPRC': {'value': 0.21246811829090967, 'ci_lower': 0.20720884997169625, 'ci_upper': 0.21918368491950427, 'mean': 0.21269243782212743, 'std': 0.0031134432661680195}, 'TP': {'value': 13995, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}, 'FP': {'value': 170556, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}, 'TN': {'value': 365177, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}, 'FN': {'value': 5228, 'ci_lower': nan, 'ci_upper': nan, 'mean': nan, 'std': nan}}\n"
     ]
    }
   ],
   "source": [
    "print( \"US: \\n\") \n",
    "print(calculate_all_metrics(np.array(all_preds)[:, 1].reshape(-1), np.array(all_labels).reshape(-1), threshold=-1.4, n_bootstrap=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93861c37-13f3-4ab7-bb7b-6380a56f3137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
