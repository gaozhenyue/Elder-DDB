{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63389587-3e69-4acf-b67d-dad3238c6c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time#可以用来简单地记录时间\n",
    "import matplotlib.pyplot as plt#画图\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  average_precision_score\n",
    "\n",
    "import torch#深度学习的pytoch平台\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528835f6-22ca-4cc8-bd3f-54387697048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.lstm import LSTMModel\n",
    "# from trainer.train_and_evaluate import ModelTrainer, ModelEvaluator\n",
    "# from data_provider.dataset_generate import TimeSeriesDataset\n",
    "from models.lstm_itransformer import lstm_itransformerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f504f-a9a9-4f0e-85e9-22cadd95aeab",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65a60c2-fa40-434d-b299-870f77a6877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimic ['ams', 'zhejiang', 'inspire', 'salz', 'eicu']\n"
     ]
    }
   ],
   "source": [
    "# database_names_all = ['ams', 'eicu', 'inspire', 'mimiciii', 'mimiciv', 'salz', 'zhejiang']\n",
    "database_names_all = ['ams', 'zhejiang', 'inspire', 'salz','eicu', 'mimic']\n",
    "# database_names_all = ['mimiciv']\n",
    "\n",
    "selected_database_num = -1\n",
    "internal_database = database_names_all[selected_database_num]  # ''\n",
    "external_database = database_names_all.copy()\n",
    "external_database.remove(internal_database) # []\n",
    "print(internal_database, external_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c468382-1bd9-40f2-9437-4d1cd3a0a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"E:\\\\Research\\\\Time series research\\\\Federated learning Time series research\\\\0.data\\\\Multi-center time series data\\\\\"\n",
    "internal_data_path = 'icu_mortality_' + internal_database + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70ad32a-c331-4cab-844a-9026b4c9bfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>bmi</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>death_hosp</th>\n",
       "      <th>los_icu_day</th>\n",
       "      <th>hr</th>\n",
       "      <th>...</th>\n",
       "      <th>alkaline_phosphatase</th>\n",
       "      <th>lymphocytes</th>\n",
       "      <th>bicarbonate</th>\n",
       "      <th>urineoutput</th>\n",
       "      <th>ventilation</th>\n",
       "      <th>norepinephrine</th>\n",
       "      <th>epinephrine</th>\n",
       "      <th>dobutamine</th>\n",
       "      <th>dopamine</th>\n",
       "      <th>hr_encode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.793136</td>\n",
       "      <td>-0.999870</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.463749</td>\n",
       "      <td>-1.139631</td>\n",
       "      <td>-0.161607</td>\n",
       "      <td>-0.036645</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.793136</td>\n",
       "      <td>-0.999870</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.463749</td>\n",
       "      <td>-1.139631</td>\n",
       "      <td>-0.161607</td>\n",
       "      <td>-0.380820</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.793136</td>\n",
       "      <td>-0.999870</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.463749</td>\n",
       "      <td>-1.139631</td>\n",
       "      <td>-0.161607</td>\n",
       "      <td>-0.380820</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.793136</td>\n",
       "      <td>-0.999870</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.463749</td>\n",
       "      <td>-1.139631</td>\n",
       "      <td>-0.161607</td>\n",
       "      <td>-0.208732</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.793136</td>\n",
       "      <td>-0.999870</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.463749</td>\n",
       "      <td>-1.139631</td>\n",
       "      <td>-0.161607</td>\n",
       "      <td>-0.251754</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918410</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.542062</td>\n",
       "      <td>-0.932103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106978</td>\n",
       "      <td>0.270036</td>\n",
       "      <td>0.036672</td>\n",
       "      <td>2.200495</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918411</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.542062</td>\n",
       "      <td>-0.932103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106978</td>\n",
       "      <td>0.270036</td>\n",
       "      <td>0.036672</td>\n",
       "      <td>1.985385</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918412</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.542062</td>\n",
       "      <td>-0.932103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106978</td>\n",
       "      <td>0.270036</td>\n",
       "      <td>0.036672</td>\n",
       "      <td>0.608684</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918413</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.542062</td>\n",
       "      <td>-0.932103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106978</td>\n",
       "      <td>0.270036</td>\n",
       "      <td>0.036672</td>\n",
       "      <td>-0.595929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918414</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.542062</td>\n",
       "      <td>-0.932103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106978</td>\n",
       "      <td>0.270036</td>\n",
       "      <td>0.036672</td>\n",
       "      <td>-0.595929</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3918415 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  gender       age    height    weight       bmi  \\\n",
       "0            5       0  1.017520 -0.021828 -0.793136 -0.999870   \n",
       "1            5       0  1.017520 -0.021828 -0.793136 -0.999870   \n",
       "2            5       0  1.017520 -0.021828 -0.793136 -0.999870   \n",
       "3            5       0  1.017520 -0.021828 -0.793136 -0.999870   \n",
       "4            5       0  1.017520 -0.021828 -0.793136 -0.999870   \n",
       "...        ...     ...       ...       ...       ...       ...   \n",
       "3918410  76271       1 -0.637632  0.583969 -0.542062 -0.932103   \n",
       "3918411  76271       1 -0.637632  0.583969 -0.542062 -0.932103   \n",
       "3918412  76271       1 -0.637632  0.583969 -0.542062 -0.932103   \n",
       "3918413  76271       1 -0.637632  0.583969 -0.542062 -0.932103   \n",
       "3918414  76271       1 -0.637632  0.583969 -0.542062 -0.932103   \n",
       "\n",
       "         admission_type  death_hosp  los_icu_day  hr  ...  \\\n",
       "0                     1           0         1.73   1  ...   \n",
       "1                     1           0         1.73   2  ...   \n",
       "2                     1           0         1.73   3  ...   \n",
       "3                     1           0         1.73   4  ...   \n",
       "4                     1           0         1.73   5  ...   \n",
       "...                 ...         ...          ...  ..  ...   \n",
       "3918410               0           0         1.17  25  ...   \n",
       "3918411               0           0         1.17  26  ...   \n",
       "3918412               0           0         1.17  27  ...   \n",
       "3918413               0           0         1.17  28  ...   \n",
       "3918414               0           0         1.17  29  ...   \n",
       "\n",
       "         alkaline_phosphatase  lymphocytes  bicarbonate  urineoutput  \\\n",
       "0                   -0.463749    -1.139631    -0.161607    -0.036645   \n",
       "1                   -0.463749    -1.139631    -0.161607    -0.380820   \n",
       "2                   -0.463749    -1.139631    -0.161607    -0.380820   \n",
       "3                   -0.463749    -1.139631    -0.161607    -0.208732   \n",
       "4                   -0.463749    -1.139631    -0.161607    -0.251754   \n",
       "...                       ...          ...          ...          ...   \n",
       "3918410             -0.106978     0.270036     0.036672     2.200495   \n",
       "3918411             -0.106978     0.270036     0.036672     1.985385   \n",
       "3918412             -0.106978     0.270036     0.036672     0.608684   \n",
       "3918413             -0.106978     0.270036     0.036672    -0.595929   \n",
       "3918414             -0.106978     0.270036     0.036672    -0.595929   \n",
       "\n",
       "         ventilation  norepinephrine  epinephrine  dobutamine  dopamine  \\\n",
       "0                  1               0            0           0         0   \n",
       "1                  1               0            0           0         0   \n",
       "2                  1               0            0           0         0   \n",
       "3                  1               0            0           0         0   \n",
       "4                  1               0            0           0         0   \n",
       "...              ...             ...          ...         ...       ...   \n",
       "3918410            0               0            0           0         0   \n",
       "3918411            0               0            0           0         0   \n",
       "3918412            0               0            0           0         0   \n",
       "3918413            0               0            0           0         0   \n",
       "3918414            0               0            0           0         0   \n",
       "\n",
       "         hr_encode  \n",
       "0         0.001389  \n",
       "1         0.002778  \n",
       "2         0.004167  \n",
       "3         0.005556  \n",
       "4         0.006944  \n",
       "...            ...  \n",
       "3918410   0.034722  \n",
       "3918411   0.036111  \n",
       "3918412   0.037500  \n",
       "3918413   0.038889  \n",
       "3918414   0.040278  \n",
       "\n",
       "[3918415 rows x 60 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_internal = pd.read_csv(file_path + internal_data_path)\n",
    "df_internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49a59dd6-26f0-407e-aaf0-834a6b7441f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "death_hosp\n",
      "0    35436\n",
      "1     5614\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_internal.groupby('id')['death_hosp'].last().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96bd7a3-75d0-4d7c-9a14-99851e806406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40804, 52798, 37874, ..., 24565, 47758, 43247], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids, test_ids = train_test_split(\n",
    "    df_internal['id'].unique(),  # 按患者ID划分\n",
    "    test_size=0.2,      # 测试集比例\n",
    "    random_state=42,    # 随机种子\n",
    "    stratify=df_internal.groupby('id')['death_hosp'].last()  # 按患者最终标签分层\n",
    ")\n",
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b962adc-8e05-4878-a6aa-cbe648ec2ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32840, 8210)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ids), len(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a24066f-3eb9-40d9-b062-9234992a00a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集比例:\n",
      "death_hosp\n",
      "0    0.863246\n",
      "1    0.136754\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "测试集比例:\n",
      "death_hosp\n",
      "0    0.863216\n",
      "1    0.136784\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 获取划分后的DataFrame\n",
    "train_df = df_internal[df_internal['id'].isin(train_ids)]\n",
    "test_df = df_internal[df_internal['id'].isin(test_ids)]\n",
    "\n",
    "print(\"训练集比例:\")\n",
    "print(train_df.groupby('id')['death_hosp'].last().value_counts(normalize=True))\n",
    "print(\"\\n测试集比例:\")\n",
    "print(test_df.groupby('id')['death_hosp'].last().value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0da16dc5-4732-4af2-a690-f733a46a454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集:\n",
      "death_hosp\n",
      "0    28349\n",
      "1     4491\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集:\n",
      "death_hosp\n",
      "0    7087\n",
      "1    1123\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集:\")\n",
    "print(train_df.groupby('id')['death_hosp'].last().value_counts())\n",
    "print(\"\\n测试集:\")\n",
    "print(test_df.groupby('id')['death_hosp'].last().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637fd405-4d60-49bb-9112-9726ed85bf31",
   "metadata": {},
   "source": [
    "## 数据生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d5f274-b09e-4143-b150-f440b59c82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, window_size=24, forecast_horizon=24, stride=1, \n",
    "                 mode='sliding', shuffle=True, label=['death_hosp'], random_state=42, max_len=None):\n",
    "        \"\"\"\n",
    "        初始化时间序列数据集\n",
    "        \n",
    "        参数:\n",
    "            df: 包含所有数据的DataFrame\n",
    "            feature_cols: 使用的特征列名列表\n",
    "            window_size: 输入序列长度\n",
    "            forecast_horizon: 预测时间范围\n",
    "            mode: 'sliding'滑动窗口，'cumulative'累积窗口，‘fix’固定时间窗口\n",
    "            shuffle: 是否打乱数据顺序\n",
    "            random_state: 随机种子\n",
    "            label: 标签\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.mode = mode\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.max_len = max_len\n",
    "        self.indices = []\n",
    "        self.stride = stride\n",
    "        self.label = label\n",
    "        \n",
    "\n",
    "        # if len(label)==1:\n",
    "        #     self.label = label[0]\n",
    "        # else:\n",
    "        #     self.label = label\n",
    "        \n",
    "        # 预计算所有可能的序列索引\n",
    "        self._precompute_indices()\n",
    "        \n",
    "    def _precompute_indices(self):\n",
    "        \"\"\"计算所有有效的序列索引\"\"\"\n",
    "        random.seed(self.random_state)\n",
    "        \n",
    "        for pid, group in tqdm(self.df.groupby('id')):\n",
    "            group = group.sort_values('hr')\n",
    "            max_hr = group['hr'].max()\n",
    "            \n",
    "            if self.mode == 'sliding':\n",
    "                for start in range(1, max_hr - self.window_size, self.stride):\n",
    "                    end = start + self.window_size\n",
    "                    forecast_end = end + self.forecast_horizon\n",
    "                    if len(group[(group['hr'] >= start) & (group['hr'] < end)]) == self.window_size:\n",
    "                        y = []\n",
    "                        for label in self.label:\n",
    "                            condition = (group['hr'] >= end) & (group['hr'] < forecast_end) & (group[label] == 1)\n",
    "                            y.append(int(condition.any()))\n",
    "                        self.indices.append((pid, start, end, y))\n",
    "                        \n",
    "            elif self.mode == 'cumulative':\n",
    "                # for end in range(max_hr, max_hr+1, self.stride):\n",
    "                # for end in range(13, 14, self.stride):\n",
    "                for end in range(1, max_hr + 1, self.stride):\n",
    "                    start = 1\n",
    "                    forecast_end = end + self.forecast_horizon\n",
    "                    y = []\n",
    "                    for label in self.label:\n",
    "                        condition = (group['hr'] >= end) & (group['hr'] <= forecast_end) & (group[label] == 1)\n",
    "                        y.append(int(condition.any()))\n",
    "                    self.indices.append((pid, start, end, y))\n",
    "                    \n",
    "            elif self.mode == 'fix':\n",
    "                start = 1\n",
    "                end = start + self.window_size\n",
    "                if len(group[(group['hr'] >= start) & (group['hr'] < end)]) == self.window_size:\n",
    "                    y = []\n",
    "                    for label in self.label:\n",
    "                        condition = (group['hr'] >= end) & (group[label] == 1)\n",
    "                        y.append(int(condition.any()))\n",
    "                    self.indices.append((pid, start, end, y))\n",
    "                        \n",
    "                    # # condition = (group['hr'] >= end) & (group['death_hosp'] == 1)\n",
    "                    # condition = group['death_hosp'] == 1\n",
    "                    # y = int(condition.any())\n",
    "                    # self.indices.append((pid, start, end, y))\n",
    "                \n",
    "        \n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pid, start, end, y = self.indices[idx]\n",
    "        group = self.df[self.df['id'] == pid].sort_values('hr')\n",
    "        \n",
    "        # 获取特征序列\n",
    "        X = group[(group['hr'] >= start) & (group['hr'] <= end)][self.feature_cols].values\n",
    "        \n",
    "        # 转换为torch张量\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "        if self.max_len is not None:\n",
    "            X_tensor = X_tensor[-self.max_len:]  # 截断到最大长度\n",
    "            \n",
    "        seq_len = len(X_tensor)\n",
    "        \n",
    "        return X_tensor, y_tensor, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c523ae2c-6f1d-4407-aa16-3759e4d27e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_cols = ['heart_rate', 'sbp', 'mbp', 'dbp', 'resp_rate', 'temperature', 'spo2', 'albumin', 'aniongap', 'bun', 'calcium', 'chloride', \n",
    "                'creatinine', 'glucose', 'sodium', 'potassium', 'fibrinogen', 'inr', 'pt', 'ptt', 'hematocrit', 'hemoglobin', 'platelet', 'wbc', \n",
    "                'alt', 'ast', 'bilirubin', 'pao2', 'paco2', 'fio2', 'pao2fio2ratio', 'ph', 'baseexcess', 'lactate', 'sao2', 'troponin', 'magnesium', \n",
    "                'bnp', 'neutrophils', 'gcs', 'alkaline_phosphatase', 'norepinephrine', 'epinephrine', 'dobutamine', 'dopamine', 'ventilation',\n",
    "                'lymphocytes', 'bicarbonate', 'urineoutput',]\n",
    "\n",
    "mask_cols = [i+'_mask' for i in mask_cols]\n",
    "# mask_cols = mask_cols + ['hr_encode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b85c7e48-5bc8-4546-a859-61bb91f34b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 32840/32840 [01:06<00:00, 490.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 8210/8210 [00:16<00:00, 486.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# 定义特征列\n",
    "feature_cols = ['gender', 'age', 'height', 'weight',\n",
    "                'heart_rate', 'sbp', 'mbp', 'resp_rate', 'temperature', 'spo2', 'albumin', 'aniongap', 'bun', 'calcium', 'chloride', \n",
    "                'creatinine', 'glucose', 'sodium', 'potassium', 'fibrinogen', 'inr', 'pt', 'ptt', 'hematocrit', 'hemoglobin', 'platelet', 'wbc', \n",
    "                'alt', 'ast', 'bilirubin', 'pao2', 'paco2', 'fio2', 'pao2fio2ratio', 'ph', 'baseexcess', 'lactate', 'sao2', 'troponin', 'magnesium', \n",
    "                'bnp', 'neutrophils', 'gcs', 'alkaline_phosphatase', 'norepinephrine', 'epinephrine', 'dobutamine', 'dopamine', 'ventilation',\n",
    "                'lymphocytes', 'bicarbonate', 'urineoutput', #'hr_encode',\n",
    "               ]\n",
    "# # 定义特征列\n",
    "# feature_cols = ['gender', 'ventilation', 'norepinephrine', 'epinephrine', 'dobutamine', 'dopamine', 'admission_type',\n",
    "#                 'age', 'height', 'weight', 'bmi',\n",
    "#                 'heart_rate', 'sbp', 'mbp', 'dbp', 'resp_rate', 'temperature', 'spo2', 'albumin', 'aniongap', 'bun', 'calcium', 'chloride', \n",
    "#                 'creatinine', 'glucose', 'sodium', 'potassium', 'fibrinogen', 'inr', 'pt', 'ptt', 'hematocrit', 'hemoglobin', 'platelet', 'wbc', \n",
    "#                 'alt', 'ast', 'bilirubin', 'pao2', 'paco2', 'fio2', 'pao2fio2ratio', 'ph', 'baseexcess', 'lactate', 'sao2', 'troponin', 'magnesium', \n",
    "#                 'bnp', 'neutrophils', 'gcs', 'alkaline_phosphatase', 'norepinephrine', 'epinephrine', 'dobutamine', 'dopamine', \n",
    "#                 'lymphocytes', 'bicarbonate', 'urineoutput',\n",
    "#                ]\n",
    "# feature_cols = feature_cols + mask_cols    \n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "# train_dataset = TimeSeriesDataset(train_df, feature_cols, window_size=24, forecast_horizon=24, mode='sliding', shuffle=True)\n",
    "# test_dataset = TimeSeriesDataset(test_df, feature_cols, window_size=24, forecast_horizon=24, mode='sliding', shuffle=False)\n",
    "\n",
    "# train_dataset = TimeSeriesDataset(train_df, feature_cols, window_size=24, mode='fix', shuffle=False)\n",
    "# test_dataset = TimeSeriesDataset(test_df, feature_cols, window_size=24, mode='fix', shuffle=False)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_df, feature_cols, mode='cumulative', shuffle=False, stride=4, label=['death_hosp'])\n",
    "test_dataset = TimeSeriesDataset(test_df, feature_cols, mode='cumulative', shuffle=False, stride=4, label=['death_hosp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35769a8f-61b9-48ab-997e-8ef0d3f7f61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "使用numpy统计:\n",
      "0: 764577 个 (96.5%)\n",
      "1: 27870 个 (3.5%)\n"
     ]
    }
   ],
   "source": [
    "# 提取标签列\n",
    "ys = np.array([y[0] for (pid, start, end, y) in train_dataset.indices])\n",
    "unique, counts = np.unique(ys, return_counts=True)\n",
    "label_dist = dict(zip(unique, counts))\n",
    "\n",
    "print(\"\\n使用numpy统计:\")\n",
    "for label, count in label_dist.items():\n",
    "    print(f\"{label}: {count} 个 ({(count/len(ys))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2b55c64-68a8-41b9-844f-e214e578bc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "使用numpy统计:\n",
      "0: 194489 个 (96.5%)\n",
      "1: 6973 个 (3.5%)\n"
     ]
    }
   ],
   "source": [
    "# 提取标签列\n",
    "ys = np.array([y[0] for (pid, start, end, y) in test_dataset.indices])\n",
    "unique, counts = np.unique(ys, return_counts=True)\n",
    "label_dist = dict(zip(unique, counts))\n",
    "\n",
    "print(\"\\n使用numpy统计:\")\n",
    "for label, count in label_dist.items():\n",
    "    print(f\"{label}: {count} 个 ({(count/len(ys))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f2d105-17ce-44c9-aac0-9d16cb969b85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 1, 1, [0]),\n",
       " (5, 1, 5, [0]),\n",
       " (5, 1, 9, [0]),\n",
       " (5, 1, 13, [0]),\n",
       " (5, 1, 17, [0]),\n",
       " (5, 1, 21, [0]),\n",
       " (5, 1, 25, [0]),\n",
       " (5, 1, 29, [0]),\n",
       " (5, 1, 33, [0]),\n",
       " (5, 1, 37, [0]),\n",
       " (5, 1, 41, [0]),\n",
       " (10, 1, 1, [0]),\n",
       " (10, 1, 5, [0]),\n",
       " (10, 1, 9, [0]),\n",
       " (10, 1, 13, [0]),\n",
       " (10, 1, 17, [0]),\n",
       " (10, 1, 21, [0]),\n",
       " (10, 1, 25, [0]),\n",
       " (10, 1, 29, [0]),\n",
       " (10, 1, 33, [0]),\n",
       " (10, 1, 37, [0]),\n",
       " (10, 1, 41, [0]),\n",
       " (10, 1, 45, [0]),\n",
       " (10, 1, 49, [0]),\n",
       " (10, 1, 53, [0]),\n",
       " (10, 1, 57, [0]),\n",
       " (10, 1, 61, [0]),\n",
       " (10, 1, 65, [0]),\n",
       " (10, 1, 69, [0]),\n",
       " (10, 1, 73, [0]),\n",
       " (10, 1, 77, [0]),\n",
       " (10, 1, 81, [0]),\n",
       " (10, 1, 85, [0]),\n",
       " (10, 1, 89, [0]),\n",
       " (10, 1, 93, [0]),\n",
       " (10, 1, 97, [0]),\n",
       " (10, 1, 101, [0]),\n",
       " (10, 1, 105, [0]),\n",
       " (10, 1, 109, [0]),\n",
       " (10, 1, 113, [0]),\n",
       " (10, 1, 117, [0]),\n",
       " (10, 1, 121, [0]),\n",
       " (10, 1, 125, [0]),\n",
       " (10, 1, 129, [0]),\n",
       " (10, 1, 133, [0]),\n",
       " (10, 1, 137, [0]),\n",
       " (10, 1, 141, [0]),\n",
       " (10, 1, 145, [0]),\n",
       " (10, 1, 149, [0]),\n",
       " (10, 1, 153, [0]),\n",
       " (11, 1, 1, [0]),\n",
       " (11, 1, 5, [0]),\n",
       " (11, 1, 9, [0]),\n",
       " (11, 1, 13, [0]),\n",
       " (11, 1, 17, [0]),\n",
       " (11, 1, 21, [0]),\n",
       " (11, 1, 25, [0]),\n",
       " (11, 1, 29, [0]),\n",
       " (11, 1, 33, [0]),\n",
       " (11, 1, 37, [0]),\n",
       " (11, 1, 41, [0]),\n",
       " (11, 1, 45, [0]),\n",
       " (11, 1, 49, [0]),\n",
       " (11, 1, 53, [0]),\n",
       " (12, 1, 1, [0]),\n",
       " (12, 1, 5, [0]),\n",
       " (12, 1, 9, [0]),\n",
       " (12, 1, 13, [0]),\n",
       " (12, 1, 17, [0]),\n",
       " (12, 1, 21, [0]),\n",
       " (12, 1, 25, [0]),\n",
       " (13, 1, 1, [0]),\n",
       " (13, 1, 5, [0]),\n",
       " (13, 1, 9, [0]),\n",
       " (13, 1, 13, [0]),\n",
       " (13, 1, 17, [0]),\n",
       " (13, 1, 21, [0]),\n",
       " (13, 1, 25, [0]),\n",
       " (13, 1, 29, [0]),\n",
       " (13, 1, 33, [0]),\n",
       " (13, 1, 37, [0]),\n",
       " (13, 1, 41, [0]),\n",
       " (13, 1, 45, [0]),\n",
       " (13, 1, 49, [0]),\n",
       " (13, 1, 53, [0]),\n",
       " (13, 1, 57, [0]),\n",
       " (13, 1, 61, [0]),\n",
       " (13, 1, 65, [0]),\n",
       " (13, 1, 69, [0]),\n",
       " (13, 1, 73, [0]),\n",
       " (13, 1, 77, [0]),\n",
       " (13, 1, 81, [0]),\n",
       " (13, 1, 85, [0]),\n",
       " (13, 1, 89, [0]),\n",
       " (13, 1, 93, [0]),\n",
       " (13, 1, 97, [0]),\n",
       " (13, 1, 101, [0]),\n",
       " (13, 1, 105, [0]),\n",
       " (13, 1, 109, [0]),\n",
       " (13, 1, 113, [0])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.indices[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d69b9e7-180d-43bf-bb1b-5f4cb3b04b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    自定义collate函数处理变长序列，生成padding mask\n",
    "    \"\"\"\n",
    "    sequences, targets, lengths = zip(*batch)\n",
    "    # print(targets)\n",
    "    \n",
    "    # 按序列长度排序(降序)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    lengths, sort_idx = lengths.sort(descending=True)\n",
    "    sequences = [sequences[i] for i in sort_idx]\n",
    "    targets = torch.stack([targets[i] for i in sort_idx])\n",
    "    \n",
    "    # 填充序列 (batch_first=True)\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # 创建padding mask (1表示有效数据，0表示padding)\n",
    "    batch_size, max_len = sequences_padded.shape[0], sequences_padded.shape[1]\n",
    "    padding_mask = torch.arange(max_len).expand(batch_size, max_len) < lengths.unsqueeze(1)\n",
    "    padding_mask = padding_mask.float().unsqueeze(-1)  # (batch_size, max_len, 1)\n",
    "    \n",
    "    return sequences_padded, targets, padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "070f7480-6fc8-4967-9759-b5af74312095",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "064a71b5-3aa0-4305-9de9-488319b86f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792447"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset) # 305866 （24-）/ 400904（1-） --stride 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ee12fb9-d0b3-49d1-ae64-f0af934d1358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 341, 52])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "for x, y, padding_mask in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0debf98e-623a-448a-83c3-505a440444fc",
   "metadata": {},
   "source": [
    "## 检查设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e70b8633-5430-47c3-93bf-1a29ce46429d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are %d GPU(s) available. 1\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080\n"
     ]
    }
   ],
   "source": [
    "# Check device \n",
    "# Get the GPU device name if available.\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available. {}'.format(torch.cuda.device_count()))\n",
    "    print('We will use the GPU: {}'.format(torch.cuda.get_device_name(0)))\n",
    "\n",
    "# If we dont have GPU but a CPU, training will take place on CPU instead\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "    \n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a13d75-6661-4946-9932-4a3ab80bcb01",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ab5802d-4070-4240-b6ac-c2a2e30e7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3, num_class=1):\n",
    "        \"\"\"\n",
    "        LSTM模型初始化\n",
    "        \n",
    "        参数:\n",
    "            input_size: 输入特征维度\n",
    "            hidden_size: 隐藏层大小\n",
    "            num_layers: LSTM层数\n",
    "            dropout: Dropout比率\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, self.num_class)\n",
    "        \n",
    "    def forward(self, x, padding_mask):\n",
    "        # LSTM层\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # # 只取最后一个时间步的输出\n",
    "        # last_out = lstm_out[:, -1, :]\n",
    "\n",
    "        # lstm_out = lstm_out * padding_mask\n",
    "        # 取每个序列最后一个非padding位置的输出\n",
    "        batch_size = x.size(0)\n",
    "        lengths = padding_mask.squeeze(-1).sum(dim=1).long()  # 各序列实际长度\n",
    "        last_out = lstm_out[torch.arange(batch_size), lengths-1, :]  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Dropout和全连接层\n",
    "        out = self.dropout(last_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        if self.num_class == 1:\n",
    "            out = torch.sigmoid(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99b61bb8-6ee6-4b63-a2dc-0d7229a73aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "# model = LSTMModel(input_size=len(feature_cols), hidden_size=256, num_layers=2, dropout=0.1, num_class=1)\n",
    "model = lstm_itransformerModel(seq_len=24, d_model=64, d_ff=64, e_layers=3, lstm_layers=2, enc_in=len(feature_cols), num_class=2)\n",
    "# model = lstm_itransformerModel(seq_len=24, d_model=128, d_ff=256, e_layers=3, enc_in=len(feature_cols), lstm_layers=2, num_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2876898-bf56-43e2-80da-ee200fff8f0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "558a2917-fed3-4f58-a769-5811854613cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.to(device)\n",
    "# for X, y, padding_mask in train_loader:\n",
    "#     X, y = X.to(device), y.to(device)\n",
    "    \n",
    "#     padding_mask = padding_mask.to(device)\n",
    "            \n",
    "#     # 前向传播\n",
    "#     outputs = model(X,padding_mask)\n",
    "#     print(X)\n",
    "#     print(X.shape)\n",
    "#     print(y.shape)\n",
    "#     print(outputs)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0a8e8ec-7fca-4da7-9822-ea8e9774fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()(outputs, y.long().squeeze(-1))\n",
    "# loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ae8725-cf57-4e13-ac61-81affabcbb23",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea557ca5-e3d8-4804-82cd-12bbe2421247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  average_precision_score\n",
    "\n",
    "# 训练与评估框架\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer,device='cuda'):\n",
    "        \"\"\"\n",
    "        初始化训练器\n",
    "        参数:\n",
    "            model: 模型实例\n",
    "            train_loader: 训练数据加载器\n",
    "            val_loader: 验证数据加载器\n",
    "            device: 训练设备\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.best_auc = 0\n",
    "        self.best_model = None\n",
    "        \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"训练一个epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(self.train_loader, desc=f\"Epoch {epoch + 1} [Train]\")\n",
    "        \n",
    "        for X, y, padding_mask in progress_bar:\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            padding_mask = padding_mask.to(self.device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = self.model(X,padding_mask)\n",
    "            if y.shape[1] == 1:\n",
    "                loss = self.criterion(outputs, y.long().squeeze(-1))\n",
    "                # loss = self.criterion(outputs, y)\n",
    "            else:\n",
    "                loss = torch.tensor(0.0, device=self.device)\n",
    "                for i in range(len(outputs)):\n",
    "                    loss = loss + self.criterion(outputs[i], y[:, i].long().squeeze(-1))\n",
    "            # # loss = self.criterion(outputs, y)\n",
    "            # loss = self.criterion(outputs, y.long().squeeze(-1))\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=4.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        return total_loss / len(self.train_loader.dataset)\n",
    "    \n",
    "    def validate(self, testloader):\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        # total_loss = []\n",
    "        # preds = []\n",
    "        # trues = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y, padding_mask in tqdm(testloader, desc=\"Validating\"):\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                padding_mask = padding_mask.to(self.device)\n",
    "                outputs = self.model(X,padding_mask)\n",
    "                if y.shape[1] == 1:\n",
    "                    loss = self.criterion(outputs, y.long().squeeze(-1))\n",
    "                    # loss = self.criterion(outputs, y)\n",
    "                else:\n",
    "                    loss = torch.tensor(0.0, device=self.device)\n",
    "                    for i in range(len(outputs)):\n",
    "                        loss = loss + self.criterion(outputs[i], y[:, i].long().squeeze(-1))\n",
    "                # loss = self.criterion(outputs, y)\n",
    "                \n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "        \n",
    "        val_loss /= len(testloader.dataset)\n",
    "        val_auc = roc_auc_score(all_labels, np.array(all_preds)[:,1])\n",
    "        val_auprc = average_precision_score(all_labels, np.array(all_preds)[:,1])\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_auc > self.best_auc:\n",
    "            self.best_auc = val_auc\n",
    "            self.best_model = self.model.state_dict().copy()\n",
    "        \n",
    "        return val_loss, val_auc, val_auprc\n",
    "    \n",
    "    def train(self, num_epochs=50, early_stop_patience=5):\n",
    "        \"\"\"完整训练流程\"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_aucs = []\n",
    "        \n",
    "        no_improve = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            val_loss, val_auc, val_auprc = self.validate(self.val_loader)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_aucs.append(val_auc)\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Val AUC: {val_auc:.4f} | Val AUPRC: {val_auprc:.4f}\")\n",
    "\n",
    "            torch.save(self.model.state_dict(), f'weights_lstmitransformer_cum_24_2_{epoch}epoch.pth')\n",
    "            # # 早停机制\n",
    "            # if val_auc > self.best_auc:\n",
    "            #     no_improve = 0\n",
    "            # else:\n",
    "            #     no_improve += 1\n",
    "            #     if no_improve >= early_stop_patience:\n",
    "            #         print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            #         break\n",
    "        \n",
    "        # 加载最佳模型\n",
    "        self.model.load_state_dict(self.best_model)\n",
    "        return train_losses, val_losses, val_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd3d5d19-0ccf-4d77-82ae-b5277ba96320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss) # pt = p if target=1, else 1-p\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49ab02c8-17d0-4004-b33a-4acfd7f18634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(MultiClassFocalLoss, self).__init__()\n",
    "        self.alpha = alpha  # 这是一个权重列表，例如 [weight_for_0, weight_for_1]\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # 先计算标准的交叉熵损失\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        \n",
    "        # CrossEntropyLoss = LogSoftmax + NLLLoss。\n",
    "        # 所以我们先对输入做Softmax，得到概率p\n",
    "        p = torch.exp(-ce_loss) \n",
    "        \n",
    "        # 计算Focal Loss的调制因子 (1 - p_t)^gamma\n",
    "        # 注意：对于正确类别，p_t就是p。这里ce_loss = -log(p)，所以p = exp(-ce_loss)\n",
    "        focal_loss = ((1 - p) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "862ac4a3-da77-4c3a-a877-8a221a9e527f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000, 27.8917], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = len(ys)\n",
    "num_negative = label_dist[0]\n",
    "num_positive = label_dist[1]\n",
    "\n",
    "# 按反比设置，让正类的权重远大于负类\n",
    "weight_for_1 = num_negative / num_positive\n",
    "weight_for_0 = 1.0  # 将负类权重设为基准 1.0\n",
    "\n",
    "# 权重张量的顺序对应类别的索引 [weight_for_class_0, weight_for_class_1]\n",
    "class_weights = torch.tensor([weight_for_0, weight_for_1], dtype=torch.float).to(device)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c01a6908-f10b-434b-93f3-0d5b5e999b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|████████████████████████████████████████████| 49528/49528 [1:06:23<00:00, 12.43it/s, loss=0.0819]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 12592/12592 [03:40<00:00, 57.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 0.1147 | Val Loss: 0.1043\n",
      "Val AUC: 0.8573 | Val AUPRC: 0.4335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|█████████████████████████████████████████████| 49528/49528 [1:06:09<00:00, 12.48it/s, loss=0.299]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 12592/12592 [03:46<00:00, 55.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "Train Loss: 0.1094 | Val Loss: 0.1042\n",
      "Val AUC: 0.8597 | Val AUPRC: 0.4432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|█████████████████████████████████████████████| 49528/49528 [1:05:37<00:00, 12.58it/s, loss=0.341]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 12592/12592 [03:36<00:00, 58.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "Train Loss: 0.1071 | Val Loss: 0.1062\n",
      "Val AUC: 0.8643 | Val AUPRC: 0.4476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|████████████████████████████████████████████| 49528/49528 [1:05:39<00:00, 12.57it/s, loss=0.0322]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 12592/12592 [03:36<00:00, 58.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:\n",
      "Train Loss: 0.1054 | Val Loss: 0.1023\n",
      "Val AUC: 0.8672 | Val AUPRC: 0.4485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|████████████████████████████████████████████| 49528/49528 [1:06:19<00:00, 12.45it/s, loss=0.0601]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 12592/12592 [03:35<00:00, 58.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:\n",
      "Train Loss: 0.1040 | Val Loss: 0.1029\n",
      "Val AUC: 0.8696 | Val AUPRC: 0.4567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|█████████████████████████████████████████████| 49528/49528 [1:05:51<00:00, 12.53it/s, loss=0.153]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 12592/12592 [03:35<00:00, 58.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:\n",
      "Train Loss: 0.1027 | Val Loss: 0.1039\n",
      "Val AUC: 0.8659 | Val AUPRC: 0.4528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]: 100%|████████████████████████████████████████████| 49528/49528 [1:05:57<00:00, 12.51it/s, loss=0.0102]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 12592/12592 [03:36<00:00, 58.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:\n",
      "Train Loss: 0.1015 | Val Loss: 0.1014\n",
      "Val AUC: 0.8700 | Val AUPRC: 0.4587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]: 100%|████████████████████████████████████████████| 49528/49528 [1:05:39<00:00, 12.57it/s, loss=0.0188]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 12592/12592 [03:35<00:00, 58.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:\n",
      "Train Loss: 0.0999 | Val Loss: 0.1014\n",
      "Val AUC: 0.8721 | Val AUPRC: 0.4611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]: 100%|█████████████████████████████████████████████| 49528/49528 [1:07:41<00:00, 12.20it/s, loss=0.182]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 12592/12592 [03:37<00:00, 57.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:\n",
      "Train Loss: 0.0990 | Val Loss: 0.1026\n",
      "Val AUC: 0.8713 | Val AUPRC: 0.4526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]: 100%|███████████████████████████████████████████| 49528/49528 [1:06:05<00:00, 12.49it/s, loss=0.0545]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 12592/12592 [03:36<00:00, 58.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:\n",
      "Train Loss: 0.0974 | Val Loss: 0.1020\n",
      "Val AUC: 0.8724 | Val AUPRC: 0.4567\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor([0.9783 / (1-0.9783)]).to(device))\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion =  nn.CrossEntropyLoss(weight=class_weights)\n",
    "# 使用FocalLoss方法\n",
    "# criterion = MultiClassFocalLoss(alpha=class_weights, gamma=2.0)\n",
    "# criterion = FocalLoss(alpha=100, gamma=2) # alpha可以设置为正样本权重的近似值\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 训练模型\n",
    "trainer = ModelTrainer(model, train_loader, test_loader, criterion, optimizer)\n",
    "train_losses, val_losses, val_aucs = trainer.train(num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8482dc7c-9985-4d8e-b39b-ee7b92f6759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0013bf8-e3bc-4c97-b92f-ebdcae5d663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'weights_lstmitransformer_cum_24_2_new.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d268492-5d6b-4f67-b439-0cfb134ffc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database name: ams\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2887/2887 [00:07<00:00, 383.19it/s]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 5414/5414 [01:35<00:00, 56.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss, auroc, auprc:  0.10399491340820317 0.8294380628346677 0.3702983660061395\n",
      "Database name: zhejiang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1247/1247 [00:04<00:00, 258.33it/s]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 3717/3717 [01:11<00:00, 52.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss, auroc, auprc:  0.10282729920740874 0.8327113291620711 0.3087318947549411\n",
      "Database name: inspire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1863/1863 [00:02<00:00, 652.29it/s]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 2112/2112 [00:29<00:00, 72.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss, auroc, auprc:  0.10282826358601296 0.6940135496958147 0.11094855608568191\n",
      "Database name: salz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6373/6373 [00:14<00:00, 431.25it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 10925/10925 [03:13<00:00, 56.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss, auroc, auprc:  0.09038508891366534 0.8033063393721753 0.29497450265693365\n",
      "Database name: eicu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 36768/36768 [01:08<00:00, 534.14it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 49059/49059 [33:09<00:00, 24.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss, auroc, auprc:  0.11559077125284978 0.800404117789405 0.2229475717545797\n"
     ]
    }
   ],
   "source": [
    "for i in external_database:\n",
    "    print(\"Database name: \" + i)\n",
    "    external_data_path = 'icu_mortality_' + i + '.csv'\n",
    "    df_external = pd.read_csv(file_path + external_data_path)\n",
    "\n",
    "    external_dataset = TimeSeriesDataset(df_external, feature_cols, mode='cumulative', shuffle=False, stride=4, label=['death_hosp'])\n",
    "    # external_dataset = TimeSeriesDataset(df_external, feature_cols, window_size=24, forecast_horizon=24, mode='sliding', shuffle=False)\n",
    "    external_loader = DataLoader(external_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    total_loss, auroc, auprc = trainer.validate(external_loader)\n",
    "    print(\"total_loss, auroc, auprc: \", total_loss, auroc, auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5adf94-7451-4474-a223-146228096e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
