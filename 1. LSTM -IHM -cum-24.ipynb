{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63389587-3e69-4acf-b67d-dad3238c6c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time#可以用来简单地记录时间\n",
    "import matplotlib.pyplot as plt#画图\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  average_precision_score\n",
    "\n",
    "import torch#深度学习的pytoch平台\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528835f6-22ca-4cc8-bd3f-54387697048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.lstm import LSTMModel\n",
    "# from trainer.train_and_evaluate import ModelTrainer, ModelEvaluator\n",
    "# from data_provider.dataset_generate import TimeSeriesDataset\n",
    "from models.lstm_itransformer import lstm_itransformerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f504f-a9a9-4f0e-85e9-22cadd95aeab",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65a60c2-fa40-434d-b299-870f77a6877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimic ['ams', 'eicu', 'salz', 'inspire', 'zhejiang']\n"
     ]
    }
   ],
   "source": [
    "# database_names_all = ['ams', 'eicu', 'inspire', 'mimiciii', 'mimiciv', 'salz', 'zhejiang']\n",
    "database_names_all = ['ams', 'eicu', 'mimic','salz', 'inspire', 'zhejiang']\n",
    "\n",
    "selected_database_num = 2\n",
    "internal_database = database_names_all[selected_database_num]  # ''\n",
    "external_database = database_names_all.copy()\n",
    "external_database.remove(internal_database) # []\n",
    "print(internal_database, external_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c468382-1bd9-40f2-9437-4d1cd3a0a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"E:\\\\Research\\\\Time series research\\\\Federated learning Time series research\\\\0.data\\\\Multi-center time series data\\\\\"\n",
    "internal_data_path = 'icu_mortality_' + internal_database + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70ad32a-c331-4cab-844a-9026b4c9bfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>bmi</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>death_hosp</th>\n",
       "      <th>los_icu_day</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>...</th>\n",
       "      <th>oasis</th>\n",
       "      <th>sapsii</th>\n",
       "      <th>respiration</th>\n",
       "      <th>coagulation</th>\n",
       "      <th>liver</th>\n",
       "      <th>cardiovascular</th>\n",
       "      <th>cns</th>\n",
       "      <th>renal</th>\n",
       "      <th>sofa</th>\n",
       "      <th>mods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.79298</td>\n",
       "      <td>-0.998913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.79298</td>\n",
       "      <td>-0.998913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.79298</td>\n",
       "      <td>-0.998913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.79298</td>\n",
       "      <td>-0.998913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.017520</td>\n",
       "      <td>-0.021828</td>\n",
       "      <td>-0.79298</td>\n",
       "      <td>-0.998913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918410</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.54191</td>\n",
       "      <td>-0.931153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918411</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.54191</td>\n",
       "      <td>-0.931153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918412</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.54191</td>\n",
       "      <td>-0.931153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918413</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.54191</td>\n",
       "      <td>-0.931153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918414</th>\n",
       "      <td>76271</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.637632</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>-0.54191</td>\n",
       "      <td>-0.931153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3918415 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  gender       age    height   weight       bmi  admission_type  \\\n",
       "0            5       0  1.017520 -0.021828 -0.79298 -0.998913               1   \n",
       "1            5       0  1.017520 -0.021828 -0.79298 -0.998913               1   \n",
       "2            5       0  1.017520 -0.021828 -0.79298 -0.998913               1   \n",
       "3            5       0  1.017520 -0.021828 -0.79298 -0.998913               1   \n",
       "4            5       0  1.017520 -0.021828 -0.79298 -0.998913               1   \n",
       "...        ...     ...       ...       ...      ...       ...             ...   \n",
       "3918410  76271       1 -0.637632  0.583969 -0.54191 -0.931153               0   \n",
       "3918411  76271       1 -0.637632  0.583969 -0.54191 -0.931153               0   \n",
       "3918412  76271       1 -0.637632  0.583969 -0.54191 -0.931153               0   \n",
       "3918413  76271       1 -0.637632  0.583969 -0.54191 -0.931153               0   \n",
       "3918414  76271       1 -0.637632  0.583969 -0.54191 -0.931153               0   \n",
       "\n",
       "         death_hosp  los_icu_day ethnicity  ...  oasis  sapsii  respiration  \\\n",
       "0                 0         1.73     other  ...   33.0    35.0            0   \n",
       "1                 0         1.73     other  ...   33.0    37.0            0   \n",
       "2                 0         1.73     other  ...   33.0    37.0            0   \n",
       "3                 0         1.73     other  ...   35.0    37.0            0   \n",
       "4                 0         1.73     other  ...   33.0    35.0            0   \n",
       "...             ...          ...       ...  ...    ...     ...          ...   \n",
       "3918410           0         1.17     white  ...    6.0    21.0            2   \n",
       "3918411           0         1.17     white  ...    6.0    21.0            0   \n",
       "3918412           0         1.17     white  ...    6.0    21.0            0   \n",
       "3918413           0         1.17     white  ...    6.0    21.0            0   \n",
       "3918414           0         1.17     white  ...    6.0    21.0            0   \n",
       "\n",
       "         coagulation  liver  cardiovascular  cns  renal  sofa  mods  \n",
       "0                  1      0               1    4      4  10.0     0  \n",
       "1                  1      0               1    4      4  10.0     0  \n",
       "2                  1      0               1    4      4  10.0     0  \n",
       "3                  1      0               1    4      4  10.0     0  \n",
       "4                  1      0               1    4      4  10.0     0  \n",
       "...              ...    ...             ...  ...    ...   ...   ...  \n",
       "3918410            0      0               1    1      0   4.0     0  \n",
       "3918411            0      0               0    0      0   NaN     0  \n",
       "3918412            0      0               0    0      0   NaN     0  \n",
       "3918413            0      0               0    0      0   NaN     0  \n",
       "3918414            0      0               0    0      0   NaN     0  \n",
       "\n",
       "[3918415 rows x 76 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_internal = pd.read_csv(file_path + internal_data_path)\n",
    "df_internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49a59dd6-26f0-407e-aaf0-834a6b7441f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "death_hosp\n",
      "0    35436\n",
      "1     5614\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_internal.groupby('id')['death_hosp'].last().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96bd7a3-75d0-4d7c-9a14-99851e806406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40804, 52798, 37874, ..., 24565, 47758, 43247], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids, test_ids = train_test_split(\n",
    "    df_internal['id'].unique(),  # 按患者ID划分\n",
    "    test_size=0.2,      # 测试集比例\n",
    "    random_state=42,    # 随机种子\n",
    "    stratify=df_internal.groupby('id')['death_hosp'].last()  # 按患者最终标签分层\n",
    ")\n",
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a24066f-3eb9-40d9-b062-9234992a00a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集比例:\n",
      "death_hosp\n",
      "0    0.863246\n",
      "1    0.136754\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "测试集比例:\n",
      "death_hosp\n",
      "0    0.863216\n",
      "1    0.136784\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 获取划分后的DataFrame\n",
    "train_df = df_internal[df_internal['id'].isin(train_ids)]\n",
    "test_df = df_internal[df_internal['id'].isin(test_ids)]\n",
    "\n",
    "print(\"训练集比例:\")\n",
    "print(train_df.groupby('id')['death_hosp'].last().value_counts(normalize=True))\n",
    "print(\"\\n测试集比例:\")\n",
    "print(test_df.groupby('id')['death_hosp'].last().value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d872d93-321a-41cc-a31b-4effb99b4f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集:\n",
      "death_hosp\n",
      "0    28349\n",
      "1     4491\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集:\n",
      "death_hosp\n",
      "0    7087\n",
      "1    1123\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集:\")\n",
    "print(train_df.groupby('id')['death_hosp'].last().value_counts())\n",
    "print(\"\\n测试集:\")\n",
    "print(test_df.groupby('id')['death_hosp'].last().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637fd405-4d60-49bb-9112-9726ed85bf31",
   "metadata": {},
   "source": [
    "## 数据生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84d5f274-b09e-4143-b150-f440b59c82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, window_size=24, forecast_horizon=24, stride=1, \n",
    "                 mode='sliding', shuffle=True, label=['death_hosp'], random_state=42, max_len=None):\n",
    "        \"\"\"\n",
    "        初始化时间序列数据集\n",
    "        \n",
    "        参数:\n",
    "            df: 包含所有数据的DataFrame\n",
    "            feature_cols: 使用的特征列名列表\n",
    "            window_size: 输入序列长度\n",
    "            forecast_horizon: 预测时间范围\n",
    "            mode: 'sliding'滑动窗口，'cumulative'累积窗口，‘fix’固定时间窗口\n",
    "            shuffle: 是否打乱数据顺序\n",
    "            random_state: 随机种子\n",
    "            label: 标签\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.mode = mode\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        self.max_len = max_len\n",
    "        self.indices = []\n",
    "        self.stride = stride\n",
    "        self.label = label\n",
    "        \n",
    "\n",
    "        # if len(label)==1:\n",
    "        #     self.label = label[0]\n",
    "        # else:\n",
    "        #     self.label = label\n",
    "        \n",
    "        # 预计算所有可能的序列索引\n",
    "        self._precompute_indices()\n",
    "        \n",
    "    def _precompute_indices(self):\n",
    "        \"\"\"计算所有有效的序列索引\"\"\"\n",
    "        random.seed(self.random_state)\n",
    "        \n",
    "        for pid, group in tqdm(self.df.groupby('id')):\n",
    "            group = group.sort_values('hr')\n",
    "            max_hr = group['hr'].max()\n",
    "            \n",
    "            if self.mode == 'sliding':\n",
    "                for start in range(1, max_hr - self.window_size, self.stride):\n",
    "                    end = start + self.window_size\n",
    "                    forecast_end = end + self.forecast_horizon\n",
    "                    if len(group[(group['hr'] >= start) & (group['hr'] < end)]) == self.window_size:\n",
    "                        y = []\n",
    "                        for label in self.label:\n",
    "                            condition = (group['hr'] >= end) & (group['hr'] < forecast_end) & (group[label] == 1)\n",
    "                            y.append(int(condition.any()))\n",
    "                        self.indices.append((pid, start, end, y))\n",
    "                        \n",
    "            elif self.mode == 'cumulative':\n",
    "                # for end in range(max_hr, max_hr+1, self.stride):\n",
    "                # for end in range(13, 14, self.stride):\n",
    "                for end in range(1, max_hr + 1, self.stride):\n",
    "                    start = 1\n",
    "                    forecast_end = end + self.forecast_horizon\n",
    "                    y = []\n",
    "                    for label in self.label:\n",
    "                        condition = (group['hr'] >= end) & (group['hr'] <= forecast_end) & (group[label] == 1)\n",
    "                        y.append(int(condition.any()))\n",
    "                    self.indices.append((pid, start, end, y))\n",
    "                    \n",
    "            elif self.mode == 'fix':\n",
    "                start = 1\n",
    "                end = start + self.window_size\n",
    "                if len(group[(group['hr'] >= start) & (group['hr'] < end)]) == self.window_size:\n",
    "                    y = []\n",
    "                    for label in self.label:\n",
    "                        condition = (group['hr'] >= end) & (group[label] == 1)\n",
    "                        y.append(int(condition.any()))\n",
    "                    self.indices.append((pid, start, end, y))\n",
    "                        \n",
    "                    # # condition = (group['hr'] >= end) & (group['death_hosp'] == 1)\n",
    "                    # condition = group['death_hosp'] == 1\n",
    "                    # y = int(condition.any())\n",
    "                    # self.indices.append((pid, start, end, y))\n",
    "                \n",
    "        \n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pid, start, end, y = self.indices[idx]\n",
    "        group = self.df[self.df['id'] == pid].sort_values('hr')\n",
    "        \n",
    "        # 获取特征序列\n",
    "        X = group[(group['hr'] >= start) & (group['hr'] <= end)][self.feature_cols].values\n",
    "        \n",
    "        # 转换为torch张量\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "        if self.max_len is not None:\n",
    "            X_tensor = X_tensor[-self.max_len:]  # 截断到最大长度\n",
    "            \n",
    "        seq_len = len(X_tensor)\n",
    "        \n",
    "        return X_tensor, y_tensor, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c523ae2c-6f1d-4407-aa16-3759e4d27e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_cols = ['heart_rate', 'sbp', 'mbp', 'resp_rate', 'temperature', 'spo2', 'albumin', 'aniongap', 'bun', 'calcium', 'chloride', \n",
    "                'creatinine', 'glucose', 'sodium', 'potassium', 'fibrinogen', 'inr', 'pt', 'ptt', 'hematocrit', 'hemoglobin', 'platelet', 'wbc', \n",
    "                'alt', 'ast', 'bilirubin', 'pao2', 'paco2', 'fio2', 'pao2fio2ratio', 'ph', 'baseexcess', 'lactate', 'sao2', 'troponin', 'magnesium', \n",
    "                'bnp', 'neutrophils', 'gcs', 'alkaline_phosphatase', 'norepinephrine', 'epinephrine', 'dobutamine', 'dopamine', 'ventilation',\n",
    "                'lymphocytes', 'bicarbonate', 'urineoutput',]\n",
    "\n",
    "mask_cols = [i+'_mask' for i in mask_cols]\n",
    "# mask_cols = mask_cols + ['hr_encode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b85c7e48-5bc8-4546-a859-61bb91f34b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 32840/32840 [01:08<00:00, 476.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 8210/8210 [00:17<00:00, 466.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# 定义特征列\n",
    "feature_cols = ['gender', 'age', 'height', 'weight', 'bmi',\n",
    "                'heart_rate', 'sbp', 'mbp',  'resp_rate', 'temperature', 'spo2', 'albumin', 'aniongap', 'bun', 'calcium', 'chloride', \n",
    "                'creatinine', 'glucose', 'sodium', 'potassium', 'fibrinogen', 'inr', 'pt', 'ptt', 'hematocrit', 'hemoglobin', 'platelet', 'wbc', \n",
    "                'alt', 'ast', 'bilirubin', 'pao2', 'paco2', 'fio2', 'pao2fio2ratio', 'ph', 'baseexcess', 'lactate', 'sao2', 'troponin', 'magnesium', \n",
    "                'bnp', 'neutrophils', 'gcs', 'alkaline_phosphatase', 'norepinephrine', 'epinephrine', 'dobutamine', 'dopamine', 'ventilation',\n",
    "                'lymphocytes', 'bicarbonate', 'urineoutput', 'hr_encode',\n",
    "               ] \n",
    "# feature_cols = feature_cols + mask_cols    \n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "# train_dataset = TimeSeriesDataset(train_df, feature_cols, window_size=24, forecast_horizon=24, mode='sliding', shuffle=True)\n",
    "# test_dataset = TimeSeriesDataset(test_df, feature_cols, window_size=24, forecast_horizon=24, mode='sliding', shuffle=False)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_df, feature_cols, mode='cumulative', shuffle=False, stride=4, label=['death_hosp'])\n",
    "test_dataset = TimeSeriesDataset(test_df, feature_cols, mode='cumulative', shuffle=False, stride=4, label=['death_hosp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35769a8f-61b9-48ab-997e-8ef0d3f7f61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "使用numpy统计:\n",
      "0: 764577 个 (96.5%)\n",
      "1: 27870 个 (3.5%)\n"
     ]
    }
   ],
   "source": [
    "# 提取标签列\n",
    "ys = np.array([y[0] for (pid, start, end, y) in train_dataset.indices])\n",
    "unique, counts = np.unique(ys, return_counts=True)\n",
    "label_dist = dict(zip(unique, counts))\n",
    "\n",
    "print(\"\\n使用numpy统计:\")\n",
    "for label, count in label_dist.items():\n",
    "    print(f\"{label}: {count} 个 ({(count/len(ys))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2b55c64-68a8-41b9-844f-e214e578bc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "使用numpy统计:\n",
      "0: 194489 个 (96.5%)\n",
      "1: 6973 个 (3.5%)\n"
     ]
    }
   ],
   "source": [
    "# 提取标签列\n",
    "ys = np.array([y[0] for (pid, start, end, y) in test_dataset.indices])\n",
    "unique, counts = np.unique(ys, return_counts=True)\n",
    "label_dist = dict(zip(unique, counts))\n",
    "\n",
    "print(\"\\n使用numpy统计:\")\n",
    "for label, count in label_dist.items():\n",
    "    print(f\"{label}: {count} 个 ({(count/len(ys))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76f2d105-17ce-44c9-aac0-9d16cb969b85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 1, 1, [0]),\n",
       " (5, 1, 5, [0]),\n",
       " (5, 1, 9, [0]),\n",
       " (5, 1, 13, [0]),\n",
       " (5, 1, 17, [0]),\n",
       " (5, 1, 21, [0]),\n",
       " (5, 1, 25, [0]),\n",
       " (5, 1, 29, [0]),\n",
       " (5, 1, 33, [0]),\n",
       " (5, 1, 37, [0])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d69b9e7-180d-43bf-bb1b-5f4cb3b04b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, padding_strategy='last_value'):\n",
    "    \"\"\"\n",
    "    支持多种填充策略的collate函数\n",
    "    padding_strategy: 'last_value', 'zero', 'mean', 'repeat'\n",
    "    \"\"\"\n",
    "    sequences, targets, lengths = zip(*batch)\n",
    "    \n",
    "    # 排序\n",
    "    lengths = torch.tensor(lengths)\n",
    "    lengths, sort_idx = lengths.sort(descending=True)\n",
    "    sequences = [sequences[i] for i in sort_idx]\n",
    "    targets = torch.stack([targets[i] for i in sort_idx])\n",
    "    \n",
    "    max_len = max(lengths)\n",
    "    batch_size = len(sequences)\n",
    "    feature_dim = sequences[0].shape[-1]\n",
    "    \n",
    "    sequences_padded = torch.zeros(batch_size, max_len, feature_dim)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_len = lengths[i]\n",
    "        sequences_padded[i, :seq_len] = seq\n",
    "        \n",
    "        # 填充策略\n",
    "        if seq_len < max_len:\n",
    "            if padding_strategy == 'last_value':\n",
    "                sequences_padded[i, seq_len:] = seq[-1]  # 最后一个值\n",
    "            elif padding_strategy == 'zero':\n",
    "                sequences_padded[i, seq_len:] = 0  # 零填充\n",
    "            elif padding_strategy == 'mean':\n",
    "                sequences_padded[i, seq_len:] = seq.mean(dim=0)  # 序列均值\n",
    "            elif padding_strategy == 'repeat':\n",
    "                # 重复整个序列直到填满\n",
    "                repeat_times = (max_len - seq_len + seq_len - 1) // seq_len + 1\n",
    "                repeated = seq.repeat(repeat_times, 1)\n",
    "                sequences_padded[i, seq_len:] = repeated[:max_len - seq_len]\n",
    "    \n",
    "    # padding mask\n",
    "    padding_mask = torch.arange(max_len).expand(batch_size, max_len) < lengths.unsqueeze(1)\n",
    "    padding_mask = padding_mask.float().unsqueeze(-1)\n",
    "    \n",
    "    return sequences_padded, targets, padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "070f7480-6fc8-4967-9759-b5af74312095",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "064a71b5-3aa0-4305-9de9-488319b86f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792447"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ee12fb9-d0b3-49d1-ae64-f0af934d1358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for x, y, padding_mask in train_loader:\n",
    "#     print(x.shape)\n",
    "#     print(y)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0debf98e-623a-448a-83c3-505a440444fc",
   "metadata": {},
   "source": [
    "## 检查设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e70b8633-5430-47c3-93bf-1a29ce46429d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are %d GPU(s) available. 1\n",
      "We will use the GPU: NVIDIA GeForce RTX 2080\n"
     ]
    }
   ],
   "source": [
    "# Check device \n",
    "# Get the GPU device name if available.\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available. {}'.format(torch.cuda.device_count()))\n",
    "    print('We will use the GPU: {}'.format(torch.cuda.get_device_name(0)))\n",
    "\n",
    "# If we dont have GPU but a CPU, training will take place on CPU instead\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "    \n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a13d75-6661-4946-9932-4a3ab80bcb01",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ab5802d-4070-4240-b6ac-c2a2e30e7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3, num_class=1):\n",
    "        \"\"\"\n",
    "        LSTM模型初始化\n",
    "        \n",
    "        参数:\n",
    "            input_size: 输入特征维度\n",
    "            hidden_size: 隐藏层大小\n",
    "            num_layers: LSTM层数\n",
    "            dropout: Dropout比率\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, self.num_class)\n",
    "        \n",
    "    def forward(self, x, padding_mask):\n",
    "        # LSTM层\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # # 只取最后一个时间步的输出\n",
    "        # last_out = lstm_out[:, -1, :]\n",
    "\n",
    "        # lstm_out = lstm_out * padding_mask\n",
    "        # 取每个序列最后一个非padding位置的输出\n",
    "        batch_size = x.size(0)\n",
    "        lengths = padding_mask.squeeze(-1).sum(dim=1).long()  # 各序列实际长度\n",
    "        last_out = lstm_out[torch.arange(batch_size), lengths-1, :]  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Dropout和全连接层\n",
    "        out = self.dropout(last_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        if self.num_class == 1:\n",
    "            out = torch.sigmoid(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99b61bb8-6ee6-4b63-a2dc-0d7229a73aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(54, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化模型\n",
    "model = LSTMModel(input_size=len(feature_cols), hidden_size=128, num_layers=2, dropout=0.1, num_class=2)\n",
    "# model = lstm_itransformerModel(seq_len=24, d_model=64, d_ff=64, e_layers=3, enc_in=len(feature_cols), ,num_class=2)\n",
    "\n",
    "model_path = './weights_lstm_cum_24_2.pth'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ae8725-cf57-4e13-ac61-81affabcbb23",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea557ca5-e3d8-4804-82cd-12bbe2421247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  average_precision_score\n",
    "\n",
    "# 训练与评估框架\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer,device='cuda'):\n",
    "        \"\"\"\n",
    "        初始化训练器\n",
    "        参数:\n",
    "            model: 模型实例\n",
    "            train_loader: 训练数据加载器\n",
    "            val_loader: 验证数据加载器\n",
    "            device: 训练设备\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.best_auc = 0\n",
    "        self.best_model = None\n",
    "        \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"训练一个epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(self.train_loader, desc=f\"Epoch {epoch + 1} [Train]\")\n",
    "        \n",
    "        for X, y, padding_mask in progress_bar:\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            padding_mask = padding_mask.to(self.device)\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = self.model(X,padding_mask)\n",
    "            if y.shape[1] == 1:\n",
    "                loss = self.criterion(outputs, y.long().squeeze(-1))\n",
    "            else:\n",
    "                loss = torch.tensor(0.0, device=self.device)\n",
    "                for i in range(len(outputs)):\n",
    "                    loss = loss + self.criterion(outputs[i], y[:, i].long().squeeze(-1))\n",
    "            # loss = self.criterion(outputs, y)\n",
    "            # loss = self.criterion(outputs, y.long().squeeze(-1))\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=4.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        return total_loss / len(self.train_loader.dataset)\n",
    "    \n",
    "    def validate(self, testloader):\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        # total_loss = []\n",
    "        # preds = []\n",
    "        # trues = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y, padding_mask in tqdm(testloader, desc=\"Validating\"):\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                padding_mask = padding_mask.to(self.device)\n",
    "                outputs = self.model(X,padding_mask)\n",
    "                if y.shape[1] == 1:\n",
    "                    loss = self.criterion(outputs, y.long().squeeze(-1))\n",
    "                else:\n",
    "                    loss = torch.tensor(0.0, device=self.device)\n",
    "                    for i in range(len(outputs)):\n",
    "                        loss = loss + self.criterion(outputs[i], y[:, i].long().squeeze(-1))\n",
    "                # loss = self.criterion(outputs, y)\n",
    "                \n",
    "                val_loss += loss.item() * X.size(0)\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "        \n",
    "        val_loss /= len(testloader.dataset)\n",
    "        val_auc = roc_auc_score(all_labels, np.array(all_preds)[:,1], average='macro')\n",
    "        val_auprc = average_precision_score(all_labels, np.array(all_preds)[:,1])\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_auc > self.best_auc:\n",
    "            self.best_auc = val_auc\n",
    "            self.best_model = self.model.state_dict().copy()\n",
    "        \n",
    "        return val_loss, val_auc, val_auprc, np.array(all_preds), all_labels\n",
    "    \n",
    "    def train(self, num_epochs=50, early_stop_patience=5):\n",
    "        \"\"\"完整训练流程\"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_aucs = []\n",
    "        \n",
    "        no_improve = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            val_loss, val_auc, val_auprc = self.validate(self.val_loader)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_aucs.append(val_auc)\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"Val AUC: {val_auc:.4f} | Val AUPRC: {val_auprc:.4f}\")\n",
    "            \n",
    "            # # 早停机制\n",
    "            # if val_auc > self.best_auc:\n",
    "            #     no_improve = 0\n",
    "            # else:\n",
    "            #     no_improve += 1\n",
    "            #     if no_improve >= early_stop_patience:\n",
    "            #         print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            #         break\n",
    "        \n",
    "        # 加载最佳模型\n",
    "        self.model.load_state_dict(self.best_model)\n",
    "        return train_losses, val_losses, val_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c01a6908-f10b-434b-93f3-0d5b5e999b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "# criterion = nn.BCELoss()\n",
    "criterion =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 训练模型\n",
    "trainer = ModelTrainer(model, train_loader, test_loader, criterion, optimizer)\n",
    "# train_losses, val_losses, val_aucs = trainer.train(num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e45eb0e-99ac-484c-9dce-3bfd431ab846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 3148/3148 [03:22<00:00, 15.51it/s]\n"
     ]
    }
   ],
   "source": [
    "total_loss, auroc, auprc, all_preds, all_labels = trainer.validate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8482dc7c-9985-4d8e-b39b-ee7b92f6759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0013bf8-e3bc-4c97-b92f-ebdcae5d663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'weights_lstm_cum_24_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d268492-5d6b-4f67-b439-0cfb134ffc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in external_database:\n",
    "#     print(\"Database name: \" + i)\n",
    "#     external_data_path = 'icu_mortality_' + i + '.csv'\n",
    "#     df_external = pd.read_csv(file_path + external_data_path)\n",
    "\n",
    "#     external_dataset = TimeSeriesDataset(df_external, feature_cols, mode='cumulative', shuffle=False, stride=2, label=['death_hosp'])\n",
    "#     # external_dataset = TimeSeriesDataset(df_external, feature_cols, window_size=24, forecast_horizon=24, mode='sliding', shuffle=False)\n",
    "#     external_loader = DataLoader(external_dataset, batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "#     total_loss, auroc, auprc = trainer.validate(external_loader)\n",
    "#     print(\"total_loss, auroc, auprc: \", total_loss, auroc, auprc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621b201-3e20-464f-9cff-4c77c6b4f6db",
   "metadata": {},
   "source": [
    "# 外部验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c75986db-b693-4e86-b3e9-c0903054f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (roc_auc_score, precision_recall_curve, \n",
    "                             accuracy_score, f1_score, precision_score, \n",
    "                             recall_score, confusion_matrix, average_precision_score)\n",
    "\n",
    "def calculate_all_metrics(all_preds, all_labels, threshold=0.5):\n",
    "    # 将概率转换为二分类预测\n",
    "    binary_preds = (all_preds >= threshold).astype(int)\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, binary_preds).ravel()\n",
    "    \n",
    "    # 计算各项指标\n",
    "    metrics = {}\n",
    "    \n",
    "    # AUROC\n",
    "    metrics['AUROC'] = roc_auc_score(all_labels, all_preds)\n",
    "    \n",
    "    # Sensitivity (Recall)\n",
    "    metrics['Sensitivity'] = recall_score(all_labels, binary_preds)\n",
    "    \n",
    "    # Specificity\n",
    "    metrics['Specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # Accuracy\n",
    "    metrics['Accuracy'] = accuracy_score(all_labels, binary_preds)\n",
    "    \n",
    "    # F1 score\n",
    "    metrics['F1_score'] = f1_score(all_labels, binary_preds)\n",
    "    \n",
    "    # Precision\n",
    "    metrics['Precision'] = precision_score(all_labels, binary_preds)\n",
    "    \n",
    "    # AUPRC\n",
    "    metrics['AUPRC'] = average_precision_score(all_labels, all_preds)\n",
    "    \n",
    "    # 额外添加一些可能有用的指标\n",
    "    metrics['TP'] = tp\n",
    "    metrics['FP'] = fp\n",
    "    metrics['TN'] = tn\n",
    "    metrics['FN'] = fn\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97a79d1c-e64c-4f30-81ad-bc6da180d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235970, 68)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1247/1247 [00:05<00:00, 243.75it/s]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 3717/3717 [00:56<00:00, 65.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss, auroc, auprc:  0.16621928574758776 0.7841673402908435 0.20897451726565947\n"
     ]
    }
   ],
   "source": [
    "external_data_path = 'icu_mortality_' + 'zhejiang' + '.csv'\n",
    "df_external = pd.read_csv(file_path + external_data_path)\n",
    "print(df_external.shape)\n",
    "\n",
    "# df_external = pd.concat([df_external, df_external_1], axis=0)\n",
    "# print(df_external.shape)\n",
    "\n",
    "external_dataset = TimeSeriesDataset(df_external, feature_cols, mode='cumulative', shuffle=False, stride=4, label=['death_hosp'])\n",
    "external_loader = DataLoader(external_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "total_loss, auroc, auprc, all_preds, all_labels = trainer.validate(external_loader)\n",
    "print(\"total_loss, auroc, auprc: \", total_loss, auroc, auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d17f1df-b03f-470e-bf31-01139f8403a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asia: \n",
      "\n",
      "{'AUROC': 0.7841673402908435, 'Sensitivity': 0.7200229489386116, 'Specificity': 0.703107677383592, 'Accuracy': 0.7036034369692792, 'F1_score': 0.1246461737100859, 'Precision': 0.06822877025116886, 'AUPRC': 0.20897451726565947, 'TP': 1255, 'FP': 17139, 'TN': 40589, 'FN': 488}\n"
     ]
    }
   ],
   "source": [
    "print( \"Asia: \\n\") \n",
    "print(calculate_all_metrics(np.array(all_preds)[:, 1].reshape(-1), np.array(all_labels).reshape(-1), threshold=-2.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e715511b-4b66-4171-a256-b08e98fa5575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(341758, 75)\n",
      "(685979, 75)\n",
      "(1027737, 75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9084/9084 [00:23<00:00, 386.93it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 16276/16276 [06:18<00:00, 43.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss, auroc, auprc:  0.16613505996387173 0.7592584734084125 0.19525585178148336\n"
     ]
    }
   ],
   "source": [
    "external_data_path = 'icu_mortality_' + 'ams' + '.csv'\n",
    "df_external_1 = pd.read_csv(file_path + external_data_path)\n",
    "print(df_external_1.shape)\n",
    "\n",
    "external_data_path = 'icu_mortality_' + 'salz' + '.csv'\n",
    "df_external = pd.read_csv(file_path + external_data_path)\n",
    "print(df_external.shape)\n",
    "\n",
    "# df_external = pd.concat([df_external, df_external_1], axis=0)\n",
    "# print(df_external.shape)\n",
    "\n",
    "# 计算df1的最大id，并顺延df2的id\n",
    "max_id = df_external_1['id'].max()\n",
    "df_external['id'] = df_external['id'] + max_id\n",
    "\n",
    "# 合并DataFrame\n",
    "df_external = pd.concat([df_external_1, df_external], ignore_index=True)\n",
    "print(df_external.shape)\n",
    "\n",
    "external_dataset = TimeSeriesDataset(df_external, feature_cols, mode='cumulative', shuffle=False, stride=4, label=['death_hosp'])\n",
    "external_loader = DataLoader(external_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "total_loss, auroc, auprc, all_preds, all_labels = trainer.validate(external_loader)\n",
    "print(\"total_loss, auroc, auprc: \", total_loss, auroc, auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f68091f-50a6-480f-bac6-de8d08f87dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Europe: \n",
      "\n",
      "{'AUROC': 0.7592584734084125, 'Sensitivity': 0.699469652327637, 'Specificity': 0.6798186298669295, 'Accuracy': 0.6803308590584956, 'F1_score': 0.10239158094498717, 'Precision': 0.055238848686506734, 'AUPRC': 0.19525585178148336, 'TP': 4748, 'FP': 81206, 'TN': 172419, 'FN': 2040}\n"
     ]
    }
   ],
   "source": [
    "print( \"Europe: \\n\") \n",
    "print(calculate_all_metrics(np.array(all_preds)[:, 1].reshape(-1), np.array(all_labels).reshape(-1), threshold=-2.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3b78f2b-81e4-42ca-98a2-70dd358ac1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3084332, 76)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 36768/36768 [01:08<00:00, 535.08it/s]\n",
      "Validating: 100%|████████████████████████████████████████████████████████████████| 12265/12265 [29:51<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss, auroc, auprc:  0.1568996512350814 0.7449005941099592 0.14328891976870295\n"
     ]
    }
   ],
   "source": [
    "external_data_path = 'icu_mortality_' + 'eicu' + '.csv'\n",
    "df_external = pd.read_csv(file_path + external_data_path)\n",
    "print(df_external.shape)\n",
    "\n",
    "# df_external = pd.concat([df_external, df_external_1], axis=0)\n",
    "# print(df_external.shape)\n",
    "\n",
    "external_dataset = TimeSeriesDataset(df_external, feature_cols, mode='cumulative', shuffle=False, stride=4, label=['death_hosp'])\n",
    "external_loader = DataLoader(external_dataset, batch_size=64, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "total_loss, auroc, auprc, all_preds, all_labels = trainer.validate(external_loader)\n",
    "print(\"total_loss, auroc, auprc: \", total_loss, auroc, auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5296f138-6188-4191-a13f-e970dcb2dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US: \n",
      "\n",
      "{'AUROC': 0.7449005941099592, 'Sensitivity': 0.6806397098223487, 'Specificity': 0.6797501702428623, 'Accuracy': 0.679777664251617, 'F1_score': 0.11613293433808869, 'Precision': 0.06348223896663079, 'AUPRC': 0.14328891976870295, 'TP': 16513, 'FP': 243607, 'TN': 517071, 'FN': 7748}\n"
     ]
    }
   ],
   "source": [
    "print( \"US: \\n\") \n",
    "print(calculate_all_metrics(np.array(all_preds)[:, 1].reshape(-1), np.array(all_labels).reshape(-1), threshold=-2.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "274bc076-b5ca-4248-8283-7019eeaad4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 3148/3148 [03:24<00:00, 15.39it/s]\n"
     ]
    }
   ],
   "source": [
    "total_loss, auroc, auprc, all_preds, all_labels = trainer.validate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89548efb-1681-4955-9122-1b137b4b2c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal test set: \n",
      "\n",
      "{'AUROC': 0.8030108504018683, 'Sensitivity': 0.7365552846694393, 'Specificity': 0.7128115214742222, 'Accuracy': 0.7136333402825347, 'F1_score': 0.1511388382084633, 'Precision': 0.08420914561164762, 'AUPRC': 0.31134726687206205, 'TP': 5136, 'FP': 55855, 'TN': 138634, 'FN': 1837}\n"
     ]
    }
   ],
   "source": [
    "print( \"Internal test set: \\n\") \n",
    "print(calculate_all_metrics(np.array(all_preds)[:, 1].reshape(-1), np.array(all_labels).reshape(-1), threshold=-2.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f311c13-cb3f-4142-9d04-4a8daf60ac86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
